{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "* General\n",
    "    * What is a workflow?\n",
    "    * What is Airflow?\n",
    "    * What are DAGs?\n",
    "    * How can Airflow be accessed?\n",
    "* Important Concepts\n",
    "* Airflow DAGs\n",
    "* Airflow Scheduling\n",
    "* Airflow Web UI\n",
    "* Airflow DAGs Operators (and Sensors)\n",
    "    * BashOperator()\n",
    "    * PythonOperator()\n",
    "    * EmailOperator()\n",
    "    * Sensors\n",
    "* Airflow Executors\n",
    "* Airflow Troubleshooting and Debugging\n",
    "* Airflow SLAs & Reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "* What is a workflow? \n",
    "    * In the data engineering context, `workflow` is essentially a set of steps to accomplish a data engineering task: downloading a file, copying data, filtering information, writing to a database, etc. Something similar to the ETL pipeline process.\n",
    "* What is Airflow?\n",
    "    * It's an orchestration tool, or a platform, to program `workflows`.\n",
    "    * Airflow are is responsible for:\n",
    "        * Creation (of a worflow)\n",
    "        * Scheduling (a worflow)\n",
    "        * Monitoring (a worflow)\n",
    "* What are DAGs?\n",
    "    * Airflow implements workflows as DAGs (Directed Acyclic Graphs)\n",
    "    * DAGs are a set of tasks with dependencies between them\n",
    "* How can Airflow be accessed?\n",
    "    * Via Code, via command-line, via a built-in web interface, via REST API\n",
    "* How to run a specific task?\n",
    "    * airflow tasks test <dag_id> <task_id> <execution_date>\n",
    "* How to run an entire DAG\n",
    "    * airflow dags trigger -e <execution_date> <dag_id>\n",
    "    * this runs a full DAG as if it were running on the specified date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG Code Example\n",
    "    # Within a python code, you refer to this with the 'etl_pipeline' variable identifier\n",
    "    # Within airflow shell command, you refer to this with the dag_id\n",
    "etl_dag = DAG(dag_id='etl_pipeline', default_args={\"start_date\": \"2024-01-08\"})\n",
    "\n",
    "\n",
    "# Running a workflow in Airflow\n",
    "    # Shell command: airflow tasks test <dag_id> <task_id> [execution date]\n",
    "    # Example: An Airflow DAG has a dag_id of etl_pipeline. The task_id is download_file and the start_date is 2023-01-08\n",
    "    airflow tasks test etl_pipeline download_file 2023-01-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Concepts\n",
    "* The Airflow System is the highest level of concepts.\n",
    "* The Airflow System contains \"components\"\n",
    "    * Example of \"components\":\n",
    "        * Airflow Scheduler: triggers scheduled workflows, or submit Tasks to the executor to run\n",
    "        * Airflow webserver: user interface to inspect, trigger and debug DAGs and tasks.\n",
    "* DAGs = Workflows = Collection/Set of Tasks (= Python DAG files because they are python scripts)\n",
    "    * \"DAG run\": Instance of a workflow (or a DAG) at a given point in time.\n",
    "    * Check: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html\n",
    "    * It is a Python script\n",
    "        * Check the line `dags_folder` folder setting in the `airflow.cfg` to see the location of the Python DAG files\n",
    "* Operators, Sensors, and TaskFlow are like \"functions\" (in the Airflow context) that outputs tasks.\n",
    "    * Note: We cannot call them airflow \"components\" because \"components\" are broader, like an Airflow \"Scheduler\" or \"webserver\" are components of the airflow context. \n",
    "* Tasks are instances of Operators/Sensors/or TaskFlows and are usually assigned to a python variable.\n",
    "    * You can perform multiple different tasks by using the same Operator.\n",
    "    * Different operators have different ideas\n",
    "        * BashOperator() to run bash/shell commands - expects a `bash_command`\n",
    "        * EmailOperator() to send emails - expects a `bash_command`\n",
    "        * PythonOperator() to execute python functions/scripts or callable methods - expects a `python_callable`\n",
    "        * BranchPythonOperator() to execute different tasks based on conditional logic (an output of a another task) - expects a `python_callable` (which must accept **kwargs) and a `provide_context=True`\n",
    "        * FileSensor() to check if a file exists - expects a `filepath` and might need `mode` or `poke_interval` attributes\n",
    "    * Example of tasks: running a command, sending an email, running a python script, etc.\n",
    "* Dependencies\n",
    "    * Defines the edges between components in the DAG.\n",
    "        * Example: \n",
    "            * A Component can be a task that was instantiated by a BashOperator(). Suppose you have multiple tasks.\n",
    "            * We need to define the order of execution for the tasks.\n",
    "            * For you to go from task_1 to task_2, I need to use `bitshifts` (`>>` or `<<`) between tasks. They create the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow commands\n",
    "    # In the airflow shell command: \n",
    "    airflow\n",
    "# Airflow help for the dags command:\n",
    "    airflow dags -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAGs\n",
    "* Terminology\n",
    "    * Directed: represents the inherent flow of the dependencies between components. \n",
    "    * Acyclic: each component is executed once per run (does not loop or repeat).\n",
    "    * Graph: represents the components and the relationships/dependencies between them.\n",
    "    * Tasks: something in the workflow that needs to be done. Ex: Task is an instance of an Operator.\n",
    "* Properties\n",
    "    * Are written in Python, but can use components in other languages (can include bash scripts, spark jobs, etc).\n",
    "    * Components: are tasks to be executed, such as: operators, sensors, etc. I.e., \n",
    "    * Dependencies: define the execution order and can be defined implicitly or explicitly\n",
    "* Airflow Shell Command line vs. Python command line\n",
    "    * The airflow command line is used to:\n",
    "        * start Airflow processes (ex: webserver, scheduler)\n",
    "        * manually run DAGs or tasks\n",
    "        * review logging info\n",
    "    * The python command line:\n",
    "        * to create or edit a DAG and the data code processing itself (of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG Example: Copy a file to a server prior to importing it to a database service\n",
    "\n",
    "# Step 0: it's good to know what DAGs are already defined: \n",
    "    # airflow shell: airflow dags list\n",
    "# Step 1: import the DAG object from airflow\n",
    "# Step 2: import other needed modules and create a default_arguments dict in order to organize the workflow\n",
    "    # owner: represent the name of the owner of the DAG\n",
    "    # email: owner's email for alerting purposes\n",
    "    # start_date: represents the earliest time a DAG could be run\n",
    "# Step 3: create a DAG object for your workflow\n",
    "    # define the DAG object using a python Context Manager (with DAG(...) as DAG_ALIAS)\n",
    "    # before Airflow 2, we would create an instance of a DAG without a context manager: etl_dag = DAG(...)\n",
    "# Step 4: troubleshoot any problem in the creation of a DAG by checking whether it was created:\n",
    "    # airflow shell: \n",
    "    airflow dags list\n",
    "\n",
    "# Import the DAG object\n",
    "from airflow import DAG\n",
    "\n",
    "# Import other modules\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2023, 1, 14),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "with DAG('example_etl', default_args=default_args) as etl_dag:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Scheduling\n",
    "* DAG run\n",
    "    * Instance of a workflow (or a DAG) at a given point in time\n",
    "    * can be run manually or via the schedule_interval parameter (passed when the DAG is defined)\n",
    "        * it represents how often to schedule the DAG runs (occurs between start_date and end_date)\n",
    "        * can be defined by a CRON style syntax or via built-in presets\n",
    "    * may have multiple states: running, failed, success, queued, skipped, etc\n",
    "    * you can check in the Airflow Web UI: Browser > DAG runs\n",
    "* Airflow scheduler presets:\n",
    "    * schedule_interval presets:\n",
    "        * @hourly (equivalent to CRON: `0****`)\n",
    "        * @daily (equivalent to CRON: `00***`)\n",
    "        * @weekly (equivalent to CRON: `00**0`)\n",
    "    * special presets:\n",
    "        * None: used for manually triggered workflows (it will never be scheduled)\n",
    "        * @once: scheduled only once\n",
    "* Nuance:\n",
    "    * Airflow will run the scheduled the DAG at \"start_date + schedule interval\", i.e., once the schedule interval has passed beyond the start_date\n",
    "    * Example:\n",
    "        * 'start_date': datetime(2020, 2, 25), 'schedule_interval': @daily\n",
    "        * This means the earliest starting time to run the DAG is on February 26th, 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2023, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "# Configure a schedule of every Wednesday at 12:30pm with CRON syntax\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Web UI\n",
    "* You can use either the Web UI or the command line tool\n",
    "* You can visualize the current DAGs as well as the code (in read-only mode)\n",
    "* For logging: Browse > Audit Logs > choose your Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the Airflow 'webserver'\n",
    "# You've successfully created some DAGs within Airflow using the command-line tools, but notice that it can be a bit tricky\n",
    "# to handle scheduling / troubleshooting / etc. After reading the documentation further, you realize that you'd like to access\n",
    "# the Airflow web interface. \n",
    "# For security reasons, you'd like to start the webserver on port 9090\n",
    "\n",
    "# Checking for help\n",
    "airflow webserver -h\n",
    "\n",
    "# Starting the webserver on port 9090\n",
    "airflow webserver -p 9090\n",
    "\n",
    "# How to examine any available DAGs\n",
    "    # Checking for operators: DAGs > select a DAG > Graph > check which operators are being used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAGs Operators (and Sensors)\n",
    "* Tasks are instances of Operators.\n",
    "    * Example of tasks: running a command, sending an email, running a python script, etc.\n",
    "* Operators do not share information with each other (it is possible if you want)\n",
    "* Example of Operators:\n",
    "    * EmptyOperator()\n",
    "        * Can be used to represent a task for troubleshooting\n",
    "        * Can also be used to represent a task that has not been implemented yet\n",
    "    * BashOperator()\n",
    "        * Executes a given bash command or script\n",
    "        * Can specify Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BashOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "# Example of a simple BashOperator()\n",
    "BashOperator(task_id='bash_example',\n",
    "            bash_command='echo \"Example!\"',\n",
    "            # Next line only for old Airflow\n",
    "            dag=dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a BashOperator()\n",
    "    # you've been running some scripts manually to clean data (using a script called cleanup.sh)\n",
    "    # you've realized it's becoming difficult to keep up with running everything manually, \n",
    "    # much less dealing with errors or retries. You'd like to implement a simple script as an Airflow operator.\n",
    "\n",
    "# Import the BashOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "with DAG(dag_id=\"test_dag\", default_args={\"start_date\": \"2024-01-01\"}) as analytics_dag:\n",
    "  # Define the BashOperator \n",
    "  cleanup = BashOperator(\n",
    "      task_id='cleanup_task',\n",
    "      # Define the bash_command\n",
    "      bash_command='cleanup.sh',\n",
    "  )\n",
    "\n",
    "# You have two more scripts, consolidate_data.sh and push_data.sh\n",
    "# These further process your data and copy to its final location\n",
    "\n",
    "# Define a second operator to run the `consolidate_data.sh` script\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh'\n",
    "    )\n",
    "\n",
    "# Define a third and final operator to execute the `push_data.sh` script\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BashOperator(): Airflow Tasks & Dependencies\n",
    "* Tasks\n",
    "    * Are instances of Operators, usually assigned to a python variable.\n",
    "    * Within Airflow tools, tasks are referred by the task_id.\n",
    "* Tasks dependencies \n",
    "    * Defines an order of execution. If they are not defined, tasks can be executed without guarantee of order\n",
    "    * Are referred to as:\n",
    "        * upstream task (must be completed `before` a downstream task): >> (upstream operator)\n",
    "        * or downstream tasks (must be completed `after` an upstream task): << (downstream operator)\n",
    "    * Note: >> or << are called \"bitshift operators\"\n",
    "    * Using bitshift operator ensures that the order is present in the \"DAGs > Graph\" bu a line connecting the tasks (or the instances of the Operators)\n",
    "    \n",
    "    <img src=\"sources/datacamp/img/bitshift.png\" alt=\"bitshift\" width=\"400px\">\n",
    "    <img src=\"sources/datacamp/img/dependencies.png\" alt=\"dependencies\" width=\"400px\">\n",
    "\n",
    "* Troubleshooting\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Tasks and Dependencies\n",
    "\n",
    "# Define the tasks\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     bash_command='echo 1')\n",
    "task2 = BashOperator(task_id='second_task',\n",
    "                     bash_command='echo 2')\n",
    "# Set first_task to run before second_task \n",
    "task1 >> task2   # or task2 << task1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a more complex DAG (or workflow)\n",
    "\n",
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command='wget https://salestracking/latestinfo?json'\n",
    ")\n",
    "\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "cleanup >> consolidate\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting\n",
    "    # Run the airflow dags command to see all subcommands available. Look for a subcommand to read errors and run it.\n",
    "    airflow dags\n",
    "\n",
    "    # Check the error message of the associated DAGs with errors\n",
    "    airflow dags list-import-errors\n",
    "\n",
    "    # Use cat workspace/dags/codependent.py to view the Python code\n",
    "        # ls -> airflow  config  start.sh  startup  workspace \n",
    "    cat workspace/dags/codependent.py\n",
    "\n",
    "    # Check the code below for the error at the end\n",
    "    from airflow import DAG\n",
    "    from airflow.operators.bash import BashOperator\n",
    "    from datetime import datetime\n",
    "\n",
    "    default_args = {\n",
    "    'owner': 'dsmith',\n",
    "    'start_date': datetime(2023, 2, 12),\n",
    "    'retries': 1\n",
    "    }\n",
    "\n",
    "    with DAG('codependency', default_args=default_args) as codependency_dag:\n",
    "\n",
    "    task1 = BashOperator(task_id='first_task',\n",
    "                        bash_command='echo 1',\n",
    "                        dag=codependency_dag)\n",
    "\n",
    "    task2 = BashOperator(task_id='second_task',\n",
    "                        bash_command='echo 2',\n",
    "                        dag=codependency_dag)\n",
    "\n",
    "    task3 = BashOperator(task_id='third_task',\n",
    "                        bash_command='echo 3',\n",
    "                        dag=codependency_dag)\n",
    "\n",
    "    # task1 must run before task2 which must run before task3\n",
    "    task1 >> task2\n",
    "    task2 >> task3\n",
    "    task3 >> task1 # THIS IS THE ERROR, SO IT MUST BE REMOVED. A DAG CANNOT HAVE A LOOP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PythonOperator() and EmailOperator()\n",
    "* You've implemented several Airflow tasks using the BashOperator but realize that a couple of specific tasks would be better implemented using Python. You'll implement a task to download and save a file to the system within Airflow.\n",
    "* PythonOperator(): Executes Python functions or a callable method.\n",
    "* EmailOperator(): Send an email from within an Airflow task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of PythonOperator() that writes messages to the task logs\n",
    "# Notes: \n",
    "    # The DAG process_sales_dag is already defined.\n",
    "    # this Python function is already defined: parse_file(inputfile, outputfile)\n",
    "import requests\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "# Define the method\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'}\n",
    ")\n",
    "\n",
    "# Add another Python task\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    ")\n",
    "\n",
    "# Import the Operator\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow Sensors\n",
    "* Sensor = An Airflow operator that waits for a condition to be `True`.\n",
    "    * Examples of conditions:\n",
    "        * Creation of a file\n",
    "        * Upload of a database record\n",
    "        * Response from a web request\n",
    "    * Since they can also instantiate tasks (just like Operators), you can use `bitshifts` (`>>` or `<<`) to create dependencies between them as well or between them.\n",
    "    * Examples of sensors:\n",
    "        * FileSensor: check for the existence of a file\n",
    "        * ExternalTaskSensor: wait for a task in another DAG to be done\n",
    "        * HttpSensor: request a web URL and check for content\n",
    "        * SQLSensor: request a SQL query and check for content\n",
    "* Sensor vs. Operator:\n",
    "    * Always use an Operator, unless you are:\n",
    "        * Ex: Uncertain when it will be true (you can use a sensor to check a task until it is completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Executors\n",
    "They are res* Determine the level of parallelism available on this system.\n",
    "* They run tasks\n",
    "* Examples:\n",
    "    * SequentialExecutor\n",
    "        * the default executor in airflow\n",
    "        * runs one task at a time (thus, having multiple workflows will take longer than expected)\n",
    "        * simple for debugging as it goes one by one\n",
    "        * however, not recommended for production (due to the limitations of task resources)\n",
    "    * LocalExecutor\n",
    "        * runs entirely on a single system\n",
    "        * treat tasks as processes, so can run concurrent tasks as permitted by your machine CPU/memory/etc\n",
    "        * This concurrency is the `parallelism` of the system, and it is defined by the User by using a limited or unlimited number of simultaneous tasks \n",
    "    * KubernetesExecutor\n",
    "        * Kubernetes is a Container orchestration system that allow tasks to be run on a cluster of machines.\n",
    "        * with KubernetesExecutor, multiple Airflow systems can be configured as `Workers` for a given set of Workflows/tasks.\n",
    "        * More complex as it requires methods that share DAGs between systems (git server, Network File System, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Example of Executors: Determine the level of parallelism available on this system.\n",
    "\n",
    "# Checking what is the executor being used\n",
    "    # Note that we assume that an Airflow instance is already done in your machine and that the `airflow.cfg`, where most\n",
    "# airflow configuration and settings are defined, already exists.\n",
    "    # Go to the command line and check the `airflow.cfg` file and look for \"executor =\"\n",
    "    cat airflow/airflow.cfg | grep \"executor =\"\n",
    "    # Ex of the output: \"executor = SequentialExecutor\"\n",
    "\n",
    "    # you can also do `airflow info` and look for \"executor\"\n",
    "    airflow info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of debugging the FileSensor\n",
    "# From https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html:\n",
    "    # Something that is checking every second should be in poke mode, while something that is checking every minute should be in reschedule mode.\n",
    "\n",
    "# Suppose your manager has mentioned that on some days, the workflows are taking a lot longer to finish and asks you to investigate.\n",
    "# She also mentions that the salesdata_ready.csv file is taking longer to generate these days and the time of day it is completed\n",
    "# is variable.\n",
    "\n",
    "# Determine the level of parallelism available on this system.\n",
    "    # We do 'airflow info' and we see it is using a SequentialExecutor\n",
    " \n",
    "# Then, we go to the DAG file and check the code\n",
    "    # The order is: precheck >> generate_report_task\n",
    "    # The code check if the file is available everyday at midnight (CRON: 00 ***)\n",
    "    # Then, it generates a report.\n",
    "    # However, the manager said the file can be there at any point in the day, so we need to use a mode that allows for resources to be available after it started checking.\n",
    "    # Notice that the mode in the FileSensor is 'poke' and not 'reschedule'. Thus, we need to put in reschedule for this reason above.\n",
    "    # From here (https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html) see that they say:\n",
    "        # \"reschedule: The Sensor takes up a worker slot only when it is checking, and sleeps for a set duration between checks\"\n",
    "# This will optimize resource usage.\n",
    "# Also, note that this is true given that the Executor being used is the SequentialExecutor\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2024,1,20),\n",
    "    mode='reschedule',  # HERE SHOULD BE 'reschedule' and not 'poke' like it was before!\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2024,1,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Troubleshooting and Debugging\n",
    "* Common Issues:\n",
    "    * DAG won't run on schedule\n",
    "        * Potential Issue 1: the Airflow Scheduler is not running\n",
    "            * Solution: Check if it is running. If not, go to the command line: `airflow scheduler`\n",
    "        * Potential Issue 2: 'schedule_interval' has not passed since either the 'start_date' or the last `DAG run`\n",
    "            * Solution: Modify either 'schedule_interval' or 'start_date'\n",
    "        * Potential Issue 3: the Executor does not have enough free slots to run tasks\n",
    "            * Solution 1: Change the Executor type to another one capable of more tasks (LocalExecutor, KubernetesExecutor)\n",
    "            * Solution 2: Add more system resources (RAM, CPUs)\n",
    "            * Solution 3: Change the scheduling of your DAGs\n",
    "    * DAG won't load into the system \n",
    "        * Potential Issue 1: DAG won't appear in the DAG View of the Airflow Web UI or in the `airflow dags list`output\n",
    "            * Solution 1: Check if the python file is in the expected DAGs folder\n",
    "                * Check the line `dags_folder` folder setting in the `airflow.cfg` to see the location of the Python DAG files\n",
    "                * The folder should be `absolute path`\n",
    "            * Potential Issue 1: Syntax errors in your Python code in the Python DAG files\n",
    "                * Solution 1: `airflow dags list-import-errors`\n",
    "                * Solution 2: Running the python DAG script: `python dagfile.py`\n",
    "                    * If there are no error, nothing will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging - General Steps\n",
    "\n",
    "# Example 1 - Issue: Where is the dag code?\n",
    "# Check Airflow Information via `airflow info`\n",
    "airflow info\n",
    "    # Apache Airflow\n",
    "    # version                | 2.7.1\n",
    "    # executor               | SequentialExecutor\n",
    "    # task_logging_handler   | airflow.utils.log.file_task_handler.FileTaskHandler\n",
    "    # sql_alchemy_conn       | sqlite:////home/repl/airflow/airflow.db\n",
    "    # dags_folder            | /home/repl/workspace/dags\n",
    "    # plugins_folder         | /home/repl/workspace\n",
    "    # base_log_folder        | /home/repl/airflow/logs\n",
    "    # remote_base_log_folder |\n",
    "\n",
    "# 1) What is the Executor: SequentialExecutor\n",
    "# 2) Where are the python DAG scripts -> dags_folder: /home/repl/workspace/dags\n",
    "\n",
    "# Check the information via the configuration file:\n",
    "# 1) Check the airflow.cfg and find where the python DAG scripts are \n",
    "cd airflow\n",
    "cat airflow/airflow.cfg | grep \"dags\" # -> dags_folder = /home/repl/workspace/dags\n",
    "\n",
    "# 2) Check the python DAG script via shell\n",
    "cd workspace/dags/\n",
    "nano test_dag.py\n",
    "\n",
    "# Example 2 - Issue: Where is the execute_report DAG (it does not appear in the system)?\n",
    "\n",
    "# Checking the DAGs\n",
    "airflow dags list\n",
    "    # dag_id     | filepath      | owner   | paused\n",
    "    # ===========+===============+=========+=======\n",
    "    # sample_dag | sample_dag.py | airflow | None\n",
    "\n",
    "    # Yes, it does not appear in the list.\n",
    "\n",
    "# Checking why is has not been loaded\n",
    "    # Potential Solution 1: Check if the python file is in the expected DAGs folder\n",
    "        # Expected dags folder: airflow info -> dags_folder = /home/repl/workspace/dags\n",
    "        cd workspace # -> dags  execute_report_dag.py\n",
    "        # the dag should be inside the dags folder, this is the problem\n",
    "        # now, move the file to inside the dags folder\n",
    "        # now, try: airflow dags list\n",
    "        # there is an error:\n",
    "            # Error: Failed to load all files. For details, run `airflow dags list-import-errors`\n",
    "            # dag_id     | filepath      | owner   | paused\n",
    "            # ===========+===============+=========+=======\n",
    "            # sample_dag | sample_dag.py | airflow | True \n",
    "        airflow dags list-import-errors\n",
    "            # File \"<frozen importlib._bootstrap>\", line 228,\n",
    "            #                                         | in _call_with_frames_removed                     \n",
    "            #                                         |   File                                           \n",
    "            #                                         | \"/home/repl/workspace/dags/execute_report_dag.py\"\n",
    "            #                                         | , line 18, in <module>                           \n",
    "            #                                         |     generate_report_task = BashOperator(         \n",
    "            #                                         | NameError: name 'BashOperator' is not defined\n",
    "        # now, go to the python DAG script\n",
    "        cd workspace/dags\n",
    "        nano execute_report_dag.py\n",
    "        # it seems that we forgot to add: from airflow.operators.bash import BashOperator\n",
    "        # now, check if the dag is on the list\n",
    "        airflow dags list\n",
    "            # dag_id         | filepath              | owner   | paused                                          \n",
    "            # ===============+=======================+=========+=======                                          \n",
    "            # execute_report | execute_report_dag.py | airflow | True  \n",
    "            # sample_dag     | sample_dag.py         | airflow | True \n",
    "        # Done! The mistake was corrected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow SLAs & Reporting\n",
    "\n",
    "* SLA = Service Level Agreement \n",
    "    * In the Airflow context, SLA is the amount of time a task or DAG should require to run\n",
    "    * Used to monitor and ensure that tasks complete within a certain time frame\n",
    "    * SLA Miss = a situation where a task or DAG does not meet the expected timing for the SLA\n",
    "        * Handling SLA Misses: If the SLA is missed, an email alert is sent out via the system configuration and a note is made in the log\n",
    "        * You can access them in the Airflow Web UI: Browse > SLA Misses\n",
    "    * Two example to define SLAs\n",
    "        * 1) In the task itself: `sla` argument that takes a `timedelta` object with the amount of time to pass\n",
    "        * 2) On the `default_args` dictionary: define an sla key with the value as a `timedelta` object\n",
    "    * `timedelta`: representing the allowed duration for the task\n",
    "        * it's found in the `datetime` library along with the datetime object\n",
    "        * use: from datetime import timedelta\n",
    "        * Examples:\n",
    "            * timedelta(seconds=30)\n",
    "            * timedelta(weeks=2)\n",
    "            * timedelta(days=4, hours=10, minutes=20, seconds=30)\n",
    "* Reporting\n",
    "    * Email alerting built into Airflow, with options for success/failure/error\n",
    "        * these are done in `default_args`\n",
    "        * it can also be done by using the EmailOperator\n",
    "    * Note: For this, you need to set up the Global Email Configuration, which is not on the scope here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You've successfully implemented several Airflow workflows into production, but you don't currently have any method of determining\n",
    "# if a workflow takes too long to run. After consulting with your manager and your team, you decide to implement an SLA\n",
    "# at the DAG level on a test workflow.\n",
    "\n",
    "# Recall that the SLA applies to the entire workflow and not to an individual task\n",
    "\n",
    "# Example 1 - Creating a default_args with SLA\n",
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2024, 1, 20),\n",
    "  'sla': timedelta(minutes=30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval=None)\n",
    "\n",
    "# Example 2 - Creating a task with the SLA\n",
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2024,1,20), schedule_interval=None)\n",
    "\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla=timedelta(hours=3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "# Airflow will email you with an attached report file \"monthly_report.pdf\" after the generate_report task completes.\n",
    "\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=[\"monthly_report.pdf\"],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "generate_report >> email_report\n",
    "\n",
    "# You've worked through most of the Airflow configuration for setting up your workflows,\n",
    "# but you realize you're not getting any notifications when DAG runs complete or fail. \n",
    "# You'd like to setup email alerting for the success and failure cases, but you want to send it to two addresses.\n",
    "\n",
    "# Example of python DAG script with default_args including the email definition\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': ['airflowalerts@datacamp.com', 'airflowadmin@datacamp.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_success': True\n",
    "}\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2023,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2023,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Templates\n",
    "\n",
    "* Every time a DAG with 'templated' information is executed, information is interpreted and included in the `DAG run`\n",
    "    * Recall: `DAG run`: Instance of a workflow (or a DAG) at a given point in time.\n",
    "* Thus, using Airflow Template allows for substituting information in a `DAG run`\n",
    "* It uses `JINJA` templating language\n",
    "* It allows for Airflow built-in runtime variables, for example:\n",
    "    * {{ ds }} for Execution Data in the YYYY-MM-DD format (as string and not as python datetime object)\n",
    "    * {{ ds_nodash }} for Execution Data with No Dashes in the YYYYMMDD format (as string and not as python datetime object)\n",
    "    * {{ dag }} for accessing a full DAG object\n",
    "    * {{ conf }} for accessing the current Airflow configuration within code\n",
    "    * {{ macros }} variable (a reference to the Airflow macros package which provides various useful objects /methods for Airflow templates)\n",
    "        * {{ macros.datetime }}: The datetime.datetime object\n",
    "        * {{ macros.timedelta }}: The timedelta object\n",
    "        * etc\n",
    "* Note: Why would you want to create individual tasks (ie, BashOperators) with specific parameters vs a list of files?\n",
    "    * When using a single task, all entries would succeed or fail as a single task. \n",
    "    * Separate operators allow for better monitoring and scheduling of these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using Airflow Template: \n",
    "# We want to do something simple: to echo the word \"Reading file_XYZ\" to a log or output\n",
    "\n",
    "# 1) First option: create multiple tasks using the BashOperator\n",
    "t1 = BashOperator(\n",
    "    task_id='first_task',\n",
    "    bash_command='echo \"Reading file1.txt\"',\n",
    "    dag=dag)\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='second_task',\n",
    "    bash_command='echo \"Reading file2.txt\"',\n",
    "    dag=dag)\n",
    "\n",
    "# Note: if we needed to process 100 files, we would have 100 BashOperator\n",
    "# Let's change to the Airflow Templated BashOperator, but still multiple tasks\n",
    "templated_command=\"\"\"  \n",
    "    echo \"Reading {{ params.filename }}\"\n",
    "\"\"\"\n",
    "\n",
    "t1 = BashOperator(task_id='template_task',\n",
    "                  bash_command=templated_command,\n",
    "                  params={'filename': 'file1.txt'},\n",
    "                  dag=example_dag)\n",
    "t2 = BashOperator(task_id='template_task',\n",
    "                  bash_command=templated_command,\n",
    "                  params={'filename': 'file2.txt'},\n",
    "                  dag=example_dag)\n",
    "\n",
    "# 2) Second option create one task and use a loop\n",
    "# Let's use a for loop to iterate over a list and output what  we need\n",
    "\n",
    "templated_command=\"\"\"\n",
    "{% for filename in params.filenames %}  \n",
    "    echo \"Reading {{ filename }}\"\n",
    "{% endfor %}\"\"\"\n",
    "\n",
    "t1 = BashOperator(task_id='template_task',\n",
    "                  bash_command=templated_command,\n",
    "                  params={'filenames': ['file1.txt', 'file2.txt']},\n",
    "                  dag=example_dag)\n",
    "\n",
    "# Output:\n",
    "    # Reading file1.txt\n",
    "    # Reading file2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: you decide to make some modifications to the design of your 'cleandata' workflow\n",
    "\n",
    "# You've successfully created a BashOperator that cleans a given data file by executing a script called cleandata.sh. \n",
    "# This works, but unfortunately requires the script to be run only for the current day. \n",
    "# Some of your data sources are occasionally behind by a couple of days and need to be run manually.\n",
    "\n",
    "# You successfully modify the cleandata.sh script to take one argument - the date in YYYYMMDD format. \n",
    "# Your testing works at the command-line, but you now need to implement this into your Airflow DAG. \n",
    "# For now, use the term {{ ds_nodash }} in your template - you'll see exactly what this is means later on.\n",
    "# Note that for now, we didn't need to define a params argument in the BashOperator - this is ok as Airflow\n",
    "# handles passing some data into templates automatically for us\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Create a templated command to execute\n",
    "# 'bash cleandata.sh datestring'\n",
    "templated_command=\"\"\"  \n",
    "    bash cleandata.sh {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          dag=cleandata_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You wish to build upon your previous DAG and modify the code to support two arguments - the date in YYYYMMDD format, \n",
    "# and a file name passed to the cleandata.sh script.\n",
    "# Modify the templated command to handle a second argument called filename.\n",
    "templated_command = \"\"\"\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ params.filename }}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to pass the new argument\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'salesdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'supportdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "                           \n",
    "# Set the operator dependencies\n",
    "clean_task >> clean_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a Jinja template to iterate over the files in a list and execute a bash command for each file.\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  {% for filename in params.filenames %}\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filenames': filelist},\n",
    "                          dag=cleandata_dag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Airflow Template for EmailOperator()\n",
    "from airflow import DAG\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content (no dashes for the date and username from the params)\n",
    "html_email_str = \"\"\"\n",
    "    Date: {{ ds }}\n",
    "    Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "                default_args={'start_date': datetime(2023, 4, 15)},\n",
    "                schedule_interval='@weekly')\n",
    "                \n",
    "email_task = EmailOperator(task_id='email_task',\n",
    "                           to='testuser@datacamp.com',\n",
    "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "                           html_content=html_email_str,\n",
    "                           params={'username': 'testemailuser'},\n",
    "                           dag=email_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Branching\n",
    "* Provides conditional logic based on Operators' output.\n",
    "* Uses the BranchPythonOperator()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python import BranchPythonOperator\n",
    "\n",
    "# Recall:\n",
    "    # **kwargs: reference to a keyword dictionary passed into the function\n",
    "\n",
    "# Defines a function that takes a data in the YYYYMMDD and check if it is an odd or even day\n",
    "def branch_test(**kwargs):\n",
    "    if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "        return'even_day_task'\n",
    "    else:return'odd_day_task'\n",
    "\n",
    "branch_task = BranchPythonOperator(task_id='branch_task',dag=dag,\n",
    "                                   provide_context=True,            # This tells Airflow to provide access to runtime variables and macros \n",
    "                                   python_callable=branch_test)\n",
    "\n",
    "start_task >> branch_task >> even_day_task >> even_day_task2\n",
    "              branch_task >> odd_day_task >> odd_day_task2\n",
    "\n",
    "# Below, see in the picture a case for even days.\n",
    "# Step 1: the start_task executes normally\n",
    "# Step 2: then, the branch_task checks the ds_nodash value and determines if it's an even or odd day and returns 'even_day_task'\n",
    "# Step 3: thn the even_day_task is executed, and then the even_day_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/workspace/sources/datacamp/img/airflow_branching.png\" alt=\"airflow_branching\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Airflow Branching\n",
    "# You'd like to run a different code path if the current execution date represents a new year (ie, 2020 vs 2019)\n",
    "\n",
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])  # takes only the YYYY from the current year\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_task >> current_year_task\n",
    "branch_task >> new_year_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Production Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "# There is sales data that will be uploaded to the system. Once the data is uploaded, \n",
    "# a new file should be created to kick off the full processing, but something isn't working correctly.\n",
    "\n",
    "# 1) Note that there are 3 tasks, given by the task_id in the respective Operators: sense_file, cleanup_tempfiles, run_processing.\n",
    "    # Note that the dag_id is: dag_id='etl_update'\n",
    "# 2) Let's run the sense_file in the airflow shell and look for errors\n",
    "    # airflow tasks test <dag_id> <task_id> <execution_date>: \n",
    "        # use -1 instead of a date (a shorthand for testing the task with the most recent execution date for which the task has been scheduled.\n",
    "        # Essentially, it tells Airflow to use the latest available run for that task in the DAG.)\n",
    "    airflow tasks test etl_update sense_file -1\n",
    "    # output: \"WARNING - cannot record queued_duration for task sense_file because previous state change time has not been saved\"\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from dags.process import process_data\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Update the default arguments and apply them to the DAG\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45, # the file sensor object to only look for its file every 45 seconds.\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "no_email_task = EmptyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable=check_weekend,\n",
    "                                   provide_context=True,\n",
    "                                   dag=dag)\n",
    "\n",
    "    \n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
