{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 1: Data Types\n",
    "    # ensuring categorical data makes sense with .describe()\n",
    "\n",
    "        # Print the information of ride_sharing\n",
    "        print(ride_sharing.info())\n",
    "\n",
    "        # Print summary statistics of user_type column\n",
    "        print(ride_sharing['user_type'].describe())\n",
    "\n",
    "    # ensuring integers are int and not str\n",
    "\n",
    "        # Strip duration of minutes\n",
    "        ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "        # Convert duration to integer\n",
    "        ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "        # Write an assert statement making sure of conversion\n",
    "        assert ride_sharing['duration_time'].dtype == 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 2: Data Inconsistencies \n",
    "    # Possibilities: \n",
    "    #   lower/upper cases for the same values\n",
    "    #       solution: lowercase (.str.lower()) or capitalize all values\n",
    "    #   typos\n",
    "    #   differences between the same values: \"unmarried\" or \"not married\"\n",
    "    #   trailing spaces: \" married\" or \" married \"\n",
    "    #       solution: .str.strip() without an argument\n",
    "    #    uniformity\n",
    "    #   columns related to each other don't match\n",
    "    #       solution: data integrity/cross field validation\n",
    "    #                 Ex: age and birthday columns should make sense\n",
    "    #                 Ex: columns that add together should add to the total column\n",
    "                        # Store fund columns to sum against\n",
    "                        fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "                        # Find rows where fund_columns row sum == inv_amount\n",
    "                        inv_equ = ( banking[fund_columns].sum(axis = 1) == banking['inv_amount'] )\n",
    "\n",
    "                        # Store consistent and inconsistent data\n",
    "                        consistent_inv = banking[inv_equ]\n",
    "                        inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "                        # Store consistent and inconsistent data\n",
    "                        print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 3: Cleaning Text Data\n",
    "    # solutions: \n",
    "        # replacing characters by other characters\n",
    "        # filtering the dataset based on characters\n",
    "\n",
    "# Replacing Characters by other Characters\n",
    "    # Replace \"Mr.\" with empty string \"\"\n",
    "    airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "\n",
    "    # Replace \"Ms.\" with empty string \"\"\n",
    "    airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "\n",
    "    # Assert that full_name has no honorifics\n",
    "    assert airlines['full_name'].str.contains('Ms.|Mr.').any() == False\n",
    "\n",
    "# Filtering the dataset based on characters\n",
    "    # Store length of each row in survey_response column\n",
    "    resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "    # Find rows in airlines where resp_length > 40\n",
    "    airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "    # Assert minimum survey_response length is > 40\n",
    "    assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "    # Print new survey_response column\n",
    "    print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 4: Membership (columns must have a predefined set of values)                              \n",
    "    # the idea is to ensure that a varible that has predefined values only have those values\n",
    "    # Ex: month variable is supposed to have 1-12 values\n",
    "    # Ex: a yes or no variable must have only 0-1 values\n",
    "\n",
    "    # Example\n",
    "        # Print categories DataFrame\n",
    "        print(categories[['cleanliness']])\n",
    "            #       cleanliness\n",
    "            # 0           Clean\n",
    "            # 1         Average\n",
    "            # 2  Somewhat clean\n",
    "            # 3  Somewhat dirty\n",
    "            # 4           Dirty\n",
    "\n",
    "        # Print unique values of survey columns in airlines\n",
    "        print(airlines['cleanliness'].unique())\n",
    "            # ['Clean', 'Average', 'Unacceptable', 'Somewhat clean', 'Somewhat dirty', 'Dirty']\n",
    "        \n",
    "        # Find the cleanliness category in airlines not in categories\n",
    "        cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "        print(cat_clean)\n",
    "            # {'Unacceptable'}\n",
    "        print(type(cat_clean))\n",
    "            # <class 'set'>\n",
    "\n",
    "        \n",
    "        # Find rows with that category\n",
    "        cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "        \n",
    "        # Print rows with consistent categories only\n",
    "        print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 5: Data Range\n",
    "    # The idea is to ensure that you only have the data range the business wants\n",
    "    \n",
    "        # Example: Categorical Variable with an numerical essense (convert to int and then convert it back to categorical)\n",
    "            # Convert tire_sizes to integer\n",
    "            ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "            # Set all values above 27 to 27\n",
    "            ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "            # Reconvert tire_sizes back to categorical\n",
    "            ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "            # Print tire size description\n",
    "            print(ride_sharing['tire_sizes'].describe())\n",
    "            type(ride_sharing['tire_sizes'])\n",
    "\n",
    "        # Example: a big has stored bike rides date taken today as if it were taken next year. We need to correct it.\n",
    "            # Recal that we sometimes need to convert date to a datetime object and then to a date type\n",
    "            # Convert ride_date to date\n",
    "            ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "            # Save today's date\n",
    "            today = dt.date.today()\n",
    "\n",
    "            # Set all in the future to today's date\n",
    "            ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "            # Print maximum of ride_dt column\n",
    "            print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 6: Categories Manipulation / Remapping\n",
    "    # pd.qcut()\n",
    "    # pd.cut()\n",
    "    # replace({'original_value':'new_value'})\n",
    "\n",
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 7: Date Manipulation\n",
    "    # Using String Similarity to scale the remapping process\n",
    "        # Example (establishing the cutoff point for the similarity score)\n",
    "            # Remember, when comparing a string with an array of strings using process.extract(), the output is a list of tuples\n",
    "            # where each is formatted like: (closest match, similarity score, index of match)\n",
    "\n",
    "            # Import process from thefuzz\n",
    "            from thefuzz import process\n",
    "\n",
    "            # Store the unique values of cuisine_type in unique_types\n",
    "            unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "            # Calculate similarity of 'asian' to all values of unique_types \n",
    "            print(process.extract('italian', unique_types, limit = len(unique_types)))\n",
    "                # [('italian', 100), ('italiann', 93), ('italiano', 93)]\n",
    "                # 80 seems to be a good cutoff\n",
    "            \n",
    "        # Example: Now, let's find matches with similarity scores equal to or higher than 80\n",
    "            # Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "            matches = process.extract('italian', restaurants['cuisine_type'], limit = len(restaurants['cuisine_type']))\n",
    "\n",
    "            # Inspect the first 5 matches\n",
    "            print(matches[0:5])\n",
    "            \n",
    "            # Iterate through the list of matches to italian\n",
    "            for match in matches:\n",
    "                # Check whether the similarity score is greater than or equal to 80\n",
    "                if match[1] >= 80:\n",
    "                    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "                    restaurants.loc[restaurants['cuisine_type'] == match[0]] = 'italian'\n",
    "            \n",
    "            # Inspect the final result\n",
    "            print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 8: Date Manipulation\n",
    "    # Python stores date types columns as Datetime, a special date type.\n",
    "    # Pandas convert a datetime column as 'object' by default\n",
    "    # We use parse_dates to specify which columns are of the datetime type and not dtype = {...} as with other columns with other types.\n",
    "\n",
    "    # Example:\n",
    "    # Create dict of columns to combine into new datetime column\n",
    "    datetime_cols = {\"Part2Start\": ['Part2StartDate', 'Part2StartTime']}\n",
    "\n",
    "    # Load file, supplying the dict to parse_dates\n",
    "    survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
    "                                parse_dates = datetime_cols)\n",
    "\n",
    "\n",
    "    # So far, you've parsed dates that pandas could interpret automatically. But if a date is in a non-standard format, \n",
    "    # like 19991231 for December 31, 1999, it can't be parsed at the import stage. \n",
    "    # Instead, use pd.to_datetime() to convert strings to dates after import.\n",
    "    print(survey_data['Part2EndTime'].head())\n",
    "        # 0    03292016 21:27:25\n",
    "        # 1    03292016 21:29:10\n",
    "        # 2    03292016 21:28:21\n",
    "    # The format above follows: %m%d%Y %H:%M:%S\n",
    "\n",
    "    # Parse datetimes and assign result back to Part2EndTime\n",
    "    survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"], format=\"%m%d%Y %H:%M:%S\")\n",
    "\n",
    "# Print first few values of Part2EndTime\n",
    "print(survey_data[\"Part2EndTime\"].head())\n",
    "    # 0   2016-03-29 21:27:25\n",
    "    # 1   2016-03-29 21:29:10\n",
    "    # 2   2016-03-29 21:28:21\n",
    "\n",
    "# Example:\n",
    "# We need to convert a date column to a datetime object and then to a date type\n",
    "        ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 9: Uniqueness\n",
    "    # find duplicates \n",
    "        #find duplicates based on some column and keep them all in the dataset\n",
    "        duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
    "    \n",
    "    # drop duplicates\n",
    "        # drop complete duplicates\n",
    "        ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "    # clean categories \n",
    "        # Print unique values of survey columns in airlines based on a \"dimension\" table called \"categories\"\n",
    "        print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "        \n",
    "        # Find the cleanliness category in airlines not in categories\n",
    "        cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "        print(cat_clean)\n",
    "        print(type(cat_clean))\n",
    "\n",
    "        # Find rows with that category\n",
    "        cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "        print(cat_clean_rows)\n",
    "        print(type(cat_clean_rows))\n",
    "\n",
    "        # Print rows with inconsistent category\n",
    "        print(airlines[cat_clean_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 10: Missing Data\n",
    "    #Missingness Types\n",
    "        # MCAR - Missing Completely at Random: no systematic relationship between missing data and other values, which makes us\n",
    "        #        believe that it is due to data entry errors when inputting data\n",
    "        # MAR - Missing at Random: there is a systematic relationship between missing data and other *observed* values such as CO2 data\n",
    "        #       being missing for low temperature (a negative correlation - the more missing data the lower the temperature),\n",
    "        #       which makes us believe that it is a sensor problem\n",
    "        # MNAR - Missing Not at Random: there is a systematic relationship between missing data and *unobserved* values. \n",
    "            # Example:\n",
    "            #   when it is very hot outside and the thermomether stopped working (so we don't have measures for days with high temp.)\n",
    "            #   however, notice that we can not infer this from the data because we cannot see what the missing data is (but we found out about it\n",
    "            #   and it made sense to relate to whatever variables we could not control for - because they were within the error term, hence the \n",
    "            #   unobserved idea).\n",
    "            # Example: \n",
    "            #   low values of satisfaction_score are missing because of inherently low satisfaction!\n",
    "\n",
    "    # Simple solutions: \n",
    "        # droping the records with missing values\n",
    "        # imputing/replacing the values with statistical measures (point estimates)\n",
    "\n",
    "        # Example\n",
    "            # Print number of missing values in banking\n",
    "            print(banking.isna().sum())\n",
    "\n",
    "            # Visualize missingness matrix\n",
    "            msno.matrix(banking)\n",
    "            plt.show()\n",
    "\n",
    "            # Isolate missing and non missing values of inv_amount\n",
    "            missing_investors = banking[banking['inv_amount'].isna()]\n",
    "            investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "            # Sort banking by age and visualize\n",
    "            banking_sorted = banking.sort_values(by = 'age')\n",
    "            msno.matrix(banking_sorted)\n",
    "            plt.show()\n",
    "        \n",
    "        # Example\n",
    "            # Drop missing values of cust_id\n",
    "            banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "            # Compute estimated acct_amount\n",
    "            acct_imp = banking_fullid['inv_amount'] * 5\n",
    "\n",
    "            # Impute missing acct_amount with corresponding acct_imp\n",
    "            banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "            # Print number of missing values for all columns\n",
    "            print(banking_imputed.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint 11: Missing data vs. Boolean\n",
    "# When Casting a column as boolean (for example, when importing with pandas read_xyz) it is important to check whether\n",
    "# there is missing values.\n",
    "# Some columns contain TRUE/FALSE, YES/NO, 0/1 values, which are trying to represent a column of the boolean type.\n",
    "# When you import data with Pandas, pandas try to guess, but there are issues:\n",
    "#   1) It guesses wrongly, so even a 0/1 column can be interpreted as float\n",
    "#   2) When there are missing values in a column, pandas might recode missing values/NAs/NaN as TRUE.\n",
    "\n",
    "# Load file with Yes as a True value and No as a False value and specify the types of some columns as boolean\n",
    "# the 'HasDebt' column is a 0/1 column\n",
    "# 'AttendedBootCampYesNo' is a Yes/No column\n",
    "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
    "                              dtype={\"HasDebt\": bool,\n",
    "                                    \"AttendedBootCampYesNo\": bool},\n",
    "                              true_values=[\"Yes\"],\n",
    "                              false_values=[\"No\"])\n",
    "\n",
    "# View the data\n",
    "print(survey_subset.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
