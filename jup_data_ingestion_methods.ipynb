{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Import Issues\n",
    "* Data types: \n",
    "    * Pandas infer types, but it might infer incorrectly: read_csv(dtype = {\"col\":type})\n",
    "    * Pandas infer missing values, but it might infer incorrectly: read_csv(na_values={\"col\" : 0}), where 0 should be interpreted as missing value.\n",
    "    * Lines with errors: a record could have more values than columns, so this will cause a parsing error: read_csv(error_bad_lines = False, warn_bad_lines = True), which will show messages when records are skipped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting from CSV\n",
    "* Flat Files\n",
    "* Source: https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2016-zip-code-data-soi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ZIPCODE AGI_STUB   MARS1   MARS2   NUMDEP\n",
      "0     NaN        0  825680  748830  1417040\n",
      "1   35004        0    2150    2140     3430\n",
      "2   35005        0    1340     890     2170\n",
      "3   35006        0     430     600      820\n",
      "4   35007        0    4770    5140     8840\n",
      "ZIPCODE       object\n",
      "AGI_STUB    category\n",
      "MARS1          int64\n",
      "MARS2          int64\n",
      "NUMDEP         int64\n",
      "dtype: object\n",
      "(1000, 5)\n",
      "    ZIPCODE AGI_STUB    MARS1    MARS2   NUMDEP\n",
      "0       NaN        0   825680   748830  1417040\n",
      "594     NaN        0   173420   125440   204720\n",
      "750     NaN        0  1315560  1068920  2026700\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = \"/workspace/sources/datacamp/general_datasets/us_tax_data_2016.csv\"\n",
    "\n",
    "# Create list of columns to use\n",
    "cols = [\"ZIPCODE\", \"AGI_STUB\", \"MARS1\", \"MARS2\", \"NUMDEP\"]\n",
    "\n",
    "# Create dict specifying data types for agi_stub and zipcode\n",
    "data_types = {'AGI_STUB':'category',\n",
    "\t\t\t  'ZIPCODE':str}\n",
    "\n",
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {'ZIPCODE':0}\n",
    "\n",
    "try:\n",
    "  # Set warn_bad_lines to issue warnings about bad records as well as other parameters\n",
    "  data = pd.read_csv(file_path, \n",
    "                     nrows=1000,\n",
    "                     skiprows=0,\n",
    "                     usecols=cols,\n",
    "                     dtype = data_types,\n",
    "                     na_values=null_values,\n",
    "                     on_bad_lines = 'warn')\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.errors.ParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")\n",
    "\n",
    "# Print data types of resulting frame\n",
    "print(data.dtypes.head())\n",
    "print(data.shape)\n",
    "\n",
    "# View rows with NA ZIP codes\n",
    "print(data[data[\"ZIPCODE\"].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting from Excel\n",
    "* Unlike flat files, Spreadsheets can have formatting and formulas and/or multiple spreadsheets can coexist in a workbook.\n",
    "* Source: https://github.com/freeCodeCamp/2021-new-coder-survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ONE SPREADSHEET WITHIN EXCEL\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "file_path = \"fcc_survey\"\n",
    "\n",
    "# Create string of lettered columns to load\n",
    "col_string = \"AD, AW:BA\"\n",
    "\n",
    "# Try reading the Excel file with the 'openpyxl' engine\n",
    "try:\n",
    "    survey_responses = pd.read_excel(file_path, \n",
    "                       engine='openpyxl',\n",
    "                       skiprows = 1, \n",
    "                       usecols = col_string)\n",
    "    print(\"File read successfully using openpyxl engine.\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    # If 'openpyxl' fails, try with 'xlrd'\n",
    "    try:\n",
    "        survey_responses = pd.read_excel(file_path, \n",
    "                           engine='xlrd',\n",
    "                           skiprows = 1, \n",
    "                           usecols = col_string)\n",
    "        print(\"File read successfully using xlrd engine.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"Unable to read the Excel file.\")\n",
    "\n",
    "# View the names of the columns selected\n",
    "print(survey_responses.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# MULTIPLE SPREADSHEETS WITHIN EXCEL\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "file_path = \"fcc_survey.xlsx\"\n",
    "\n",
    "# Examples to load sheets\n",
    "    # 1) Load ALL sheets in the Excel file\n",
    "    all_survey_data = pd.read_excel(\"file_path.xlsx\",\n",
    "                                    sheet_name = None)\n",
    "\n",
    "    # 2) Load all sheets in the Excel file with index and name\n",
    "    all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                    sheet_name = [0, '2017'])\n",
    "\n",
    "    # 3) Load both the 2016 and 2017 sheets by name\n",
    "    all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                    sheet_name = ['2016', '2017'])\n",
    "    # View the data type of all_survey_data\n",
    "    print(type(all_survey_data)) # type will be a dictionary where keys are the sheetnames and values are the data\n",
    "    print(all_survey_data.keys())\n",
    "\n",
    "    # 4) Create an empty dataframe to hold all loaded sheets and concatenate\n",
    "    all_responses = pd.DataFrame()\n",
    "\n",
    "    # Set up for loop to iterate through values in responses\n",
    "    for df in responses.values():\n",
    "    # Print the number of rows being added\n",
    "    print(\"Adding {} rows\".format(df.shape[0]))\n",
    "    # Concatenate all_responses and df, assign result\n",
    "    all_responses = pd.concat([all_responses, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Here we use Pandas & SQLAlchemy & Faker to ingest fake data into the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table_test']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>2016</td>\n",
       "      <td>2024-04-15 14:39:27.564142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>2016</td>\n",
       "      <td>2024-04-15 14:39:27.564142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171</td>\n",
       "      <td>2842</td>\n",
       "      <td>2024-04-15 14:39:27.564142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178</td>\n",
       "      <td>9162</td>\n",
       "      <td>2024-04-15 14:39:27.564142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117</td>\n",
       "      <td>8347</td>\n",
       "      <td>2024-04-15 14:39:27.564142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  amount                 created_at\n",
       "0       75    2016 2024-04-15 14:39:27.564142\n",
       "1       28    2016 2024-04-15 14:39:27.564142\n",
       "2      171    2842 2024-04-15 14:39:27.564142\n",
       "3      178    9162 2024-04-15 14:39:27.564142\n",
       "4      117    8347 2024-04-15 14:39:27.564142"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use the SQLAlchemy package to access an postgres database\n",
    "\n",
    "# We start by importing the create_engine function.\n",
    "    # This engine fires up a SQL engine that will communicates out SQL queries to the database \n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "# Create the engine\n",
    "engine = create_engine('postgresql://myuser:mypassword@postgres/mydatabase')\n",
    "\n",
    "# Checking the table names within the database\n",
    "insp = inspect(engine)\n",
    "print(insp.get_table_names(schema=\"schema_test\")) # recall that postgres prefer lower case for names \n",
    "\n",
    "# Connecting to the engine and executing a SELECT query\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    faker = Faker('en_US')\n",
    "\n",
    "    # Insert fake data\n",
    "    for i in range(10):\n",
    "        test_id = faker.random_int(min=1, max=200)\n",
    "        amount = faker.random_int(min=100, max=10000)\n",
    "        #created_at: recall that the created_at is defined in the init.sql\n",
    "        #insert_query = text(f\"INSERT INTO SCHEMA_TEST.TABLE_TEST (test_id, amount) VALUES ({test_id}, {amount})\")\n",
    "        insert_query = text(\"INSERT INTO SCHEMA_TEST.TABLE_TEST (test_id, amount) VALUES (:test_id, :amount)\")\n",
    "        conn.execute(insert_query, {\"test_id\": test_id, \"amount\": amount})\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit() # committing refers to finalizing and applying the changes made within a transaction to the database.\n",
    "\n",
    "    # Fetch and print the table after inserting the data\n",
    "    select_query = text(\"SELECT * FROM SCHEMA_TEST.TABLE_TEST\")\n",
    "    result = conn.execute(select_query) # Created a SQLAlchemy object that is assigned to the result variable\n",
    "    df = pd.DataFrame(result.fetchall()) # Fetches all rows\n",
    "    df.columns = result.keys() # set the dataframe column names\n",
    "    # Print the table after inserting the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Here we use Pandas & SQLAlchemy & Faker to ingest fake data into the Postgres database, but quicker at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table_test']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>1909</td>\n",
       "      <td>2024-04-15 14:50:40.478758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198</td>\n",
       "      <td>4832</td>\n",
       "      <td>2024-04-15 14:50:40.478758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173</td>\n",
       "      <td>1485</td>\n",
       "      <td>2024-04-15 14:50:40.478758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>174</td>\n",
       "      <td>929</td>\n",
       "      <td>2024-04-15 14:50:40.478758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91</td>\n",
       "      <td>1693</td>\n",
       "      <td>2024-04-15 14:50:40.478758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  amount                 created_at\n",
       "0      200    1909 2024-04-15 14:50:40.478758\n",
       "1      198    4832 2024-04-15 14:50:40.478758\n",
       "2      173    1485 2024-04-15 14:50:40.478758\n",
       "3      174     929 2024-04-15 14:50:40.478758\n",
       "4       91    1693 2024-04-15 14:50:40.478758"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use the SQLAlchemy package to access an postgres database, but with pandas at the end to query it\n",
    "\n",
    "# We start by importing the create_engine function.\n",
    "    # This engine fires up a SQL engine that will communicates out SQL queries to the database \n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "# Create the engine\n",
    "engine = create_engine('postgresql://myuser:mypassword@postgres/mydatabase')\n",
    "\n",
    "# Checking the table names within the database\n",
    "insp = inspect(engine)\n",
    "print(insp.get_table_names(schema=\"schema_test\")) # recall that postgres prefer lower case for names \n",
    "\n",
    "# Connecting to the engine and executing a SELECT query\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    faker = Faker('en_US')\n",
    "\n",
    "    # Insert fake data\n",
    "    for i in range(10):\n",
    "        test_id = faker.random_int(min=1, max=200)\n",
    "        amount = faker.random_int(min=100, max=10000)\n",
    "        #created_at: recall that the created_at is defined in the init.sql\n",
    "        #insert_query = text(f\"INSERT INTO SCHEMA_TEST.TABLE_TEST (test_id, amount) VALUES ({test_id}, {amount})\")\n",
    "        insert_query = text(\"INSERT INTO SCHEMA_TEST.TABLE_TEST (test_id, amount) VALUES (:test_id, :amount)\")\n",
    "        conn.execute(insert_query, {\"test_id\": test_id, \"amount\": amount})\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit() # committing refers to finalizing and applying the changes made within a transaction to the database.\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM SCHEMA_TEST.TABLE_TEST\", engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: Here we use Pandas & urllib to ingest CSV data from an URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4: Here we ingest data from an URL with HTTP requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"robots\" content=\"noindex,nofollow\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><style>*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .big-button,body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .big-button,body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .big-button,body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width <= 720px){.main-content{margin-top:4rem}.heading-favicon{height:1.5rem;width:1.5rem}}.footer,.main-content{padding-left:1.5rem;padding-right:1.5rem}.main-wrapper{align-items:center;display:flex;flex:1;flex-direction:column}.font-red{color:#b20f03}.spacer{margin:2rem 0}.h1{font-size:2.5rem;font-weight:500;line-height:3.75rem}.h2{font-weight:500}.core-msg,.h2{font-size:1.5rem;line-height:2.25rem}.body-text,.core-msg{font-weight:400}.body-text{font-size:1rem;line-height:1.25rem}@media (width <= 720px){.h1{font-size:1.5rem;line-height:1.75rem}.h2{font-size:1.25rem}.core-msg,.h2{line-height:1.5rem}.core-msg{font-size:1rem}}#challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+);padding-left:34px}#challenge-error-text,#challenge-success-text{background-repeat:no-repeat;background-size:contain}#challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=);padding-left:42px}.text-center{text-align:center}.big-button{border:.063rem solid #0051c3;border-radius:.313rem;font-size:.875rem;line-height:1.313rem;padding:.375rem 1rem;transition-duration:.2s;transition-property:background-color,border-color,color;transition-timing-function:ease}.big-button:hover{cursor:pointer}.captcha-prompt:not(.hidden){display:flex}@media (width <= 720px){.captcha-prompt:not(.hidden){flex-wrap:wrap;justify-content:center}}.pow-button{background-color:#0051c3;color:#fff;margin:2rem 0}.pow-button:hover{background-color:#003681;border-color:#003681;color:#fff}.footer{font-size:.75rem;line-height:1.125rem;margin:0 auto;max-width:60rem;width:100%}.footer-inner{border-top:1px solid #d9d9d9;padding-bottom:1rem;padding-top:1rem}.clearfix:after{clear:both;content:\"\";display:table}.clearfix .column{float:left;padding-right:1.5rem;width:50%}.diagnostic-wrapper{margin-bottom:.5rem}.footer .ray-id{text-align:center}.footer .ray-id code{font-family:monaco,courier,monospace}.core-msg,.zone-name-title{overflow-wrap:break-word}@media (width <= 720px){.diagnostic-wrapper{display:flex;flex-wrap:wrap;justify-content:center}.clearfix:after{clear:none;content:none;display:initial;text-align:center}.column{padding-bottom:2rem}.clearfix .column{float:none;padding:0;width:auto;word-break:keep-all}.zone-name-title{margin-bottom:1rem}}.loading-spinner{height:76.391px}.lds-ring{display:inline-block;position:relative}.lds-ring,.lds-ring div{height:1.875rem;width:1.875rem}.lds-ring div{animation:lds-ring 1.2s cubic-bezier(.5,0,.5,1) infinite;border:.3rem solid transparent;border-radius:50%;border-top-color:#313131;box-sizing:border-box;display:block;position:absolute}.lds-ring div:first-child{animation-delay:-.45s}.lds-ring div:nth-child(2){animation-delay:-.3s}.lds-ring div:nth-child(3){animation-delay:-.15s}@keyframes lds-ring{0%{transform:rotate(0)}to{transform:rotate(1turn)}}@media screen and (-ms-high-contrast:active),screen and (-ms-high-contrast:none){.main-wrapper,body{display:block}}</style><meta http-equiv=\"refresh\" content=\"105\"></head><body class=\"no-js\"><div class=\"main-wrapper\" role=\"main\"><div class=\"main-content\"><noscript><div id=\"challenge-error-title\"><div class=\"h2\"><span id=\"challenge-error-text\">Enable JavaScript and cookies to continue</span></div></div></noscript></div></div><script>(function(){window._cf_chl_opt={cvId: '3',cZone: \"www.datacamp.com\",cType: 'non-interactive',cNounce: '8781',cRay: '875c5f997ca503ce',cHash: '50b09eb08562c25',cUPMDTk: \"\\/teach\\/documentation?__cf_chl_tk=pRB2Tz41N8jgM1AyaAyE8FUXmgmGvVUS6zhgSS8PH0A-1713356471-0.0.1.1-1343\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '105000',cTplV: 5,cTplB: 'cf',cK: \"visitor-time\",fa: \"\\/teach\\/documentation?__cf_chl_f_tk=pRB2Tz41N8jgM1AyaAyE8FUXmgmGvVUS6zhgSS8PH0A-1713356471-0.0.1.1-1343\",md: \"0nAdZiX9kNPohNnvpSKsvnC3ez1NDB15tbqW39c7f00-1713356471-1.1.1.1-ZGoeyTz_2RvNKl1fg2_xR0zyFp72C8EeD65XSN1R2Hcw_HpZOLrTY.qMPslI73FXe9Gk.mQslJ2kE6mahitb6rvkwcJTW6io5vrTCwib2r5yxof83QuorG3fzt5sbQIoBrXFsUW1OIcM2jyBfaf.2zHWOGdKcbSjPrQrVAW8CWI3D4xB95xyvvN5HPrdT1l2O_nCrXdBpf7n1BImZPBh3v7QZu4Z73xAsUV6VQC_IufNcM7W8G9sbAUUltf3tXPT8BV4qlnd.mk8ycGcMHxVJ13lQDptabvmjwBr1kPpbGNo6COYEu14kVffg3V6IHLGh1bBPmhb_FsCcL13IVd.tuYlnURZERh_iTpFolFsNx8SID.x3M.tze2FOCnhOxvkWRC5jr6MrdYZlXEAjUkVcUCCsKzJ2rwNr.P7m0tklPh5dBSkpIT.d5O1rctqLZQr2P18S4H9hyjKF5HuPCpoC3rP1PA3Ea8JAD8jnuHPFtPvugGX76poYolBJvp75cu2vi7_KR8kJv4gj.iRxu3NaiudKixSOsgvqBQts3_EqaPliJFW5DTNmfy8BkY4Ar5VhsDE203ws2f0Xyo4mj6MZm2aW8ZdpYhaA1JJBkTikPSb6_vALqC5ZrT1tmq_4kreqkiaes0D8GNhDdZnYv5JpM5WHgs2JaR22SbQQyLjRotOuSzSvElTZ2vieHaL5FQCqdKkvzUfEtxBSI5mwdNuS5.vByUAHv_8xD.iekDLHvWqJx2HviUg28SHlwEINtmTfnXGUPZEPllq_rO6vqVOlR4.6QPWPzejRfZbIft0qJMZpLKvTCZu4F0An_q_mG4Bq6wacOegbnYvHE9_G.TRySnilhBAtykPS5i.6tQIqUQEYiITu6QlUD0OO9.6EBgmOCRuwJ5BwNhL5Bok88DQ_ORzQz1QvpvWcykjZWQydFb1CzDmotpEclTIGqCl6fa2I_B5QUQFLlTj1NAvVJU1kEz5tGfYLAqxKv5HhIrICKLzzy0zNNLo7Nve_sYvw4on2vW6VlufCBvGJoFnLJmLp9tlWncbCL6MNIuMRwQ9ikd4lx4WS98RTwgqU2Rpp0jE66sf896XExodshOAhP_D3vPY6gKWjKrHvv9ZW0f21.8ODkdZ7gixNnloOva4HSy1rL84GKLS6gASXGEvuQmx_0OqeiOCY39IPbT01ehBuVnhu3mZoV1n6y3c4kiVcFSp0zkOWVfu_5BXbWP_aHCMTapnrA72IFWqMNGPQ6q3lmND3SHSO.1WJw1CugFE0lKh\",mdrd: \"qwUZJe7XjqKntIsuQQMCeDR_zXASURzhb_qUPIWJ.jw-1713356471-1.1.1.1-7PhOGVF61bV.aYGo2Ht1wa917P1gQMX5FD8OBTF8wmBp_qzUQG7eMkQn.U6H2RPfJ4MBNC5j1t_sRhDfBSleUPGo.GZccaAuIu8YrdKVR6Jt63r_9uixELiEJsT5WgqCdcLlXaJq3HzD1a3uoTf.NfgeBOGv0TizW_ZZtyW9b.pwhwEOAWygCaoGqdTeBUr0TApwBLdATAKSYLzo_fkWNH3vdJ_ESo.mqUl33C5bWrwF83CK8NSFW3ibzvUBiOBiHth2MDuVWPXlWMiPcMrIiPK__tfSPzHaaQy6AmNbS9KxfmTK4FYZIyblTqtf7MV.tFHewtrT8mB_KA0rW4c13IPJI90SNhZcx0NqBeQIthitjIwFlhgcCVvR1HZKOGs2vNlrPLgUqq16.xTHAcT6IMc.KB9z.Oz9_Q7nfm8dd9xgFfitzRzQLnvuFaDhz0llUg60p4MF.y1Y69jlXlwaYO92KFVCyKrNX5IAWYDnbt_uRjB7brvnT7BOdXZtqfFynWVe0PGkqXYti_9N5P0ZCY29y5yklH5De4wK7BUUA8QJkLo9kLnsTj_akIrb2iOdqgU1yv.vpOZi2o1D4RVQdCngbqF00ZL3rY1FR8EvBds5TAxAttIULkG.msnTsrK78E1WLU8NBmoKWIushpyV97XDSepLNydU.0po5mANcZLQqqVE9gUO3R9wtJmFcvdVIjbdQi4bL1AsrCe56CINvybml7alEoqbHnbqn0YA1FqhO5OF27rrXxaVRRyGgHsdvwuazm6.zjC4w3rFsgYHuwYFgnildvmeocpsmvlTQFD8drSks.uoSGyh53JbeBuOkJEK.EME0wfcU8AQrCLqat59W1QSmISAN7woFWwfjRbiJOd7ySnAogTBrkAux0lteDmc.suVrFB57hwsbsTPEGiSvcu3VjNtbjLcVSuGayAEGLP49ugEmmqnNQ9z7CH6QoxvUd_zFcKz4TBXQmB2nN6dCpvzOxp91t1vDoukGP3OZRaut87hB3VALyEvowqDFyLWtEylD4rb3BMuQMk13EplJkZQus.JoXjSG.x3cCba1OSet6MDrJEtN.v6j.7Ss7vQYKLMz47vVgPduSA8DBOaRCuXUdcDyGR9Ss8EQeWDKASEvE_ivJv7bWp_S.0v.9jsWUQmFmdS8SvQAeG4YfPsy.mM1Ngpf2BUDvfUoroOel9CgjBHC1y2dtODbzDt2.B8kT8zwETWYhwEemQ5LZ7E6x70b0J0wfXBvvabE.KklWOFdyuH9VKIxyRU2wmMvVKifL_OGIhLVgzXZ08dQrMPZ1VOTXZGNCNfD2bbWdNo8TtwMZZHvwH7s9K3u.rioqnguVODCb3EopHblOTGImEpb93x5lqyoj6fjEYDKb.JOMnNm4UO4s7Fot8YxQSmXywaosHQ_6o0cbJwE19FFFoWVDU00xUzUQ3CqCOEpEWNkvXsU7ldSYQOLnbeVyujm5hmyWusUsBQ1iHIQzjpQL80ZcIvCAB22mFNd01S8rpLDzh2D76Nd427OuxvJcUdlJvM8tpRYnzfKRQrnc3ZV8LRDorAhsMytvbi1Hs0I0YlFntHFdGlusI_sshtXOKXwiqLIcgQX5p5b39jNRHKl7zuaWfNjaOeWsVqBq5Ht_vBDoLVWsJx8JF_Rgliq65BHjPFMSLy3wFF8TsfJhNkUyvCBUZNwumqToQYCop0VTaCUxBt97S6VpdtiR58rCNfhsmT6yriFsEL8U6SJgjqIdgMbxF4vhGAve66YAN16eCVBbzt8xEFn_0RwHfiynCt9pHKBTkN4lpW2ExREerQpsi1VPbjv2yI6Sb2tiroy3jJPbzlMXI1GI2Lqw7OWiuqnWe7ZFNCxpkk83VnHmIj_TLpKaJ5Ibg.UNN92qbv06dMC57Ufytj4ai7avglAZClFx6OTJA0TsCegW.d6JWeQ0xIqDoiXDP6xxfrlxydXWHcJ5rUiWFDCZ6FnJvU8FjlqTiNQuDDaqnSx5CZ9NbXK_a5srernxja2NNhfgRqtxibeLHOMX3SAkvFWyWqmb7UJe_XFnArllfOOByIDIqhH0cd1nMJRHJs1PkuEN58NXYmZ9rIr2MqCu28lSa2PEpJm4KPSSA_kgOS.qO8wTFZcNHI5QngHcHAMFUaWsBiVNK.ovY_YlI67rc_tbTg4ZGXeA7WuraROMq9ApY8HTIcMHjcxoches8.ibobx2OYs5Oz43DrncMotw.0mCoG0Jwvobtef3WtDOYdPein1itdb3rD0cLN6865Ez69epccsRwxXcVJi5ql44h6E8x_.gAN\",cRq: {ru: 'aHR0cHM6Ly93d3cuZGF0YWNhbXAuY29tL3RlYWNoL2RvY3VtZW50YXRpb24=',ra: 'cHl0aG9uLXJlcXVlc3RzLzIuMzEuMA==',rm: 'R0VU',d: 'f7BiNZIGFPWvlphihT2Q7Zk1UiBbv5H0gCqI+bm+KvYa7JTJPJ1p5hzWtGbCUPyAPIi/bKZIV+0b5ySoaz1LahICQUWxwINs8RkDVBQd0h3NpSnJJqHlWfehuWPUBCtPxkjwoSShU5SnUSSsLBAtDPVbtePRvmCLBwYWgjvND/krU4+EeMER2/9mizzuKIm7synrDvfrx1gV+fqQ7TemfaKyWaC0Iao57JydODvAMDm2pVD7707AwjfcZGQO8PmqSjXR7VRCC26eRlPdnOPsxa4BHTYfPJBA1F+r6b6mRlKrxYzJ03Rwg9YPNRuu3POF9DC/xjsQ7kgVp2l3otS/aXqw8ydFL1B2xAizcTNSSKYYHO0jfENm9THV3elZXXdlP0jV/SdI90p/SS+ErM5gkzKiYEd4cib3PNCQOwVr0jKkx8nlsIC0hh1cbFnougVJZNfgyjbkh7WRWLa4Ncm2a8LwDZyktqopvp0Dw+zs2Kck7WfkcJZ21e2BE3S7GdCOW7b5zmbZKF74VrvouznRvgsVE51oW1O9p6jivUQywTgQxDWjTgVMa8tlAMEF7Iyqr7HbE25+/LDbJ5TdEjZ1mDG28jm5dxQhWGnJ7TSJWPQ=',t: 'MTcxMzM1NjQ3MS4yODUwMDA=',cT: Math.floor(Date.now() / 1000),m: 'nZbZN7pfyknTa+YpeMcDTRTgCvAPKgrsQEmfnE/FHkE=',i1: 'eNh57149V0N4HRpb0bQJWA==',i2: 'TzXB6/Cbk76uO/vlNRmpjw==',zh: 'hzfiqo9hugT9sHeHQ1zy81NCL/S0295H0+GuRnkSV9o=',uh: 'YE9XOpG5TeHmhA1zfs5mxC8CrRZzq2a/+r+OU7dliYQ=',hh: 'rAZnIHiyrNuZ60h9aAZNML8izDilqmOSNuCtac1WqPs=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=875c5f997ca503ce';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/teach\\/documentation?__cf_chl_rt_tk=pRB2Tz41N8jgM1AyaAyE8FUXmgmGvVUS6zhgSS8PH0A-1713356471-0.0.1.1-1343\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());</script></body></html>\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Packages the request, send the request and catch the response r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response\n",
    "text = r.text\n",
    "\n",
    "# Print the html\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 5: Here we Scrape the web using BeautifulSoup and HTTP requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "images/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "Resume.html\n",
      "https://docs.python.org\n",
      "https://github.com/python/cpython/issues\n",
      "https://discuss.python.org\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks)\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 6: Here we Ingest data from APIs and JSONs, anonymously (without an account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The Social Network\n",
      "Year:  2010\n",
      "Rated:  PG-13\n",
      "Released:  01 Oct 2010\n",
      "Runtime:  120 min\n",
      "Genre:  Biography, Drama\n",
      "Director:  David Fincher\n",
      "Writer:  Aaron Sorkin, Ben Mezrich\n",
      "Actors:  Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot:  As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\n",
      "Language:  English, French\n",
      "Country:  United States\n",
      "Awards:  Won 3 Oscars. 173 wins & 187 nominations total\n",
      "Poster:  https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore:  95\n",
      "imdbRating:  7.8\n",
      "imdbVotes:  754,796\n",
      "imdbID:  tt1285016\n",
      "Type:  movie\n",
      "DVD:  05 Jun 2012\n",
      "BoxOffice:  $96,962,694\n",
      "Production:  N/A\n",
      "Website:  N/A\n",
      "Response:  True\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary\n",
    "json_data = r.json()\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n",
    "\n",
    "# or this: Print each key-value pair in json_data\n",
    "# for k, v in json_data.items():\n",
    "#     print(k + ': ', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 7: Here we Ingest data from APIs and nested JSONs, anonymously (without an account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1033289096\">\n",
      "<p class=\"mw-empty-elt\">\n",
      "\n",
      "</p>\n",
      "<p><b>Pizza</b> (<span></span> <i title=\"English pronunciation respelling\"><span>PEET</span>-sə</i>, <span>Italian:</span> <span lang=\"it-Latn-fonipa\">[ˈpittsa]</span>; <link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1177148991\"><span>Neapolitan:</span> <span lang=\"nap-Latn-fonipa\">[ˈpittsə]</span>) is a dish of Italian origin consisting of a flat base of leavened wheat-based dough topped with tomato, cheese, and other ingredients, baked at a high temperature, traditionally in a wood-fired oven.</p><p>The term <i>pizza</i> was first recorded in the year 997 AD, in a Latin manuscript from the southern Italian town of Gaeta, in Lazio, on the border with Campania. Raffaele Esposito is often credited for creating modern pizza in Naples. In 2009, Neapolitan pizza was registered with the European Union as a traditional speciality guaranteed dish. In 2017, the art of making Neapolitan pizza was added to UNESCO's list of intangible cultural heritage.</p><p>Pizza and its variants are among the most popular foods in the world. Pizza is sold at a variety of restaurants, including pizzerias (pizza specialty restaurants), Mediterranean restaurants, via delivery, and as street food. In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork. In casual settings, however, it is typically cut into slices to be eaten while held in the hand. Pizza is also sold in grocery stores in a variety of forms, including frozen or as kits for self-assembly. They are then cooked using a home oven.\n",
      "</p><p>In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias. Overall, 13% of the U.S. population aged two years and over consumed pizza on any given day.</p>\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract (nested jsons)\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 8: Here we Ingest data from Twitter APIs and nested JSONs, with an account (with authentication credentials). We use Tweepy and we filter tweets for specific tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tweepy' has no attribute 'Stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m access_token_secret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create your Stream object with credentials\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[43mtweepy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStream\u001b[49m(consumer_key, consumer_secret, access_token, access_token_secret)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Filter your Stream variable\u001b[39;00m\n\u001b[1;32m     16\u001b[0m stream\u001b[38;5;241m.\u001b[39mfilter([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclinton\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrump\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msanders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcruz\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tweepy' has no attribute 'Stream'"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import pandas as pd\n",
    "import tweepy #uncomment the tweepy installation in requirements.txt\n",
    "\n",
    "# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "\n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "\n",
    "myStreamListener = MyStreamListener()\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Filter your Stream variable\n",
    "stream.filter([\"clinton\", \"trump\", \"sanders\", \"cruz\"])\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets.txt'\n",
    "\n",
    "# Initialize empty list to store tweets (this will be a list of dictionaries)\n",
    "tweets_data = []\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns = ['text', 'lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Count how many tweets contain the words 'clinton', 'trump', 'sanders' and 'cruz'\n",
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 9: Here we Ingest & Stream data from APIs, with an account (with authentication credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0     1     3\n",
       "1     2     4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "d\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
