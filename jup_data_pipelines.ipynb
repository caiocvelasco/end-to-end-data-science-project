{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "* General\n",
    "    * What is a Data Pipeline?\n",
    "    * Who builds it?\n",
    "    * ETL\n",
    "    * ETL vs. ELT\n",
    "* Architecture Diagrams\n",
    "* Ensuring Quality Data Pipelines\n",
    "* Data Persistence and Data Lineage\n",
    "* Testing\n",
    "* Terminology\n",
    "* Examples of Source Systems\n",
    "* Testing Data Pipeline / Monitoring Pipeline Performance\n",
    "* Architecting a Pipeline (Testing vs. Production Envs + Orchestration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "* What is a Data Pipeline?\n",
    "    * An *automated* process for ETL.\n",
    "* Who builds it?\n",
    "    * Data Engineers / ETL Developers\n",
    "        * Python & SQL: to build stuff\n",
    "        * Airflow: to orchestrate and build production-ready data pipelines\n",
    "* ETL\n",
    "    * Extract: pull raw data from a source system (file, database or API)\n",
    "    * Transform: convert raw data into a desired data model\n",
    "    * Load: Persist data for downstream use (destinations: file, data warehouse, POST request to an API)\n",
    "* ETL vs. ELT\n",
    "    * ELT is great when data is to be loaded in Data Warehouses ot Data Marts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Diagrams\n",
    "* Data Architecture (more complete)\n",
    "\n",
    "    <img src=\"/workspace/sources/datacamp/img/architecture_01.png\" alt=\"architecture 1\" width=\"400px\">\n",
    "\n",
    "* Data ETL Pipeline (Deliver to Client)\n",
    "\n",
    "    <img src=\"/workspace/sources/datacamp/img/architecture_02.png\" alt=\"architecture 2\" width=\"400px\">\n",
    "\n",
    "* Data Ingestion ETL Pipeline (Persisting data in Data Marts for Analytics Team)\n",
    "\n",
    "    <img src=\"/workspace/sources/datacamp/img/architecture_03.png\" alt=\"architecture 3\" width=\"400px\">\n",
    "\n",
    "* ELT Pipeline (Extract from Flat File, Load into DW, then Transformed into a Reporting Model via Stored-procedure)\n",
    "\n",
    "    <img src=\"/workspace/sources/datacamp/img/architecture_04.png\" alt=\"architecture 4\" width=\"400px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring Quality Data Pipelines\n",
    "* Resilient\n",
    "    * Handle failures and automatically retry.\n",
    "* Idempotent\n",
    "    * Running a pipeline multiple times should give the same output (should not result in duplicate values).\n",
    "* Scalable\n",
    "    * Can handle large amounts of data and increased frequency of runs.\n",
    "* Transparent\n",
    "    * It's important to avoid black-box transformations on data.\n",
    "    * Data pipelines should be well-tested and documented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Persistence and Data Lineage\n",
    "* While more commonn in the Load (L) step of the pipeline, it should be used in multiple stages of the pipeline. \n",
    "* Persisting data to a fileallows for a \"snapshot\" to be taken at various points throughout the pipeline. \n",
    "    * Storing \"snapshots\" of the data (along the pipeline process) to a file or somewhere else is called Data Persistence.\n",
    "    * Pandas can use to_csv() to write a dataframe to a file. \n",
    "* Persisting data also make it easier for documenting operations applied to data throughout the pipeline \n",
    "* Snapshots\n",
    "    * Data will be eventually stored somewhere, but it is important to capture \"snapshots\" of the data along the pipeline. \n",
    "    * It allows for pipeline to be rerun from last point of failure.\n",
    "    * The green files are logical places to persist data within the E-T-L components of the pipeline. This allows us to rerun the pipeline from last point of failure rather than from the beginning, which is the basis for **Data Lineage**. \n",
    "\n",
    "    <img src=\"/workspace/sources/datacamp/img/snapshots.png\" alt=\"snapshots\" width=\"400px\">\n",
    "* Data Lineage\n",
    "    * Build upon the persistence of data to document the journey taken by the data throughout the pipeline.\n",
    "    * It increases transparency to the solutions and instills trust in data consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Engineering Best Practices: Modularity\n",
    "* Separate E, T, L in different modules, i.e., diferent functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "* Before Deployment (before shipping to production), we need to run tests, of course\n",
    "    * Unit Tests    \n",
    "        * Unit tests are used to validate the functionality and output of the code\n",
    "    * End-to-end testing\n",
    "        * The pipeline should be tested in all steps of the journey\n",
    "        * Consider testing for:\n",
    "            * Small and Large data\n",
    "            * Empty files\n",
    "            * Bad data\n",
    "    * Code Review\n",
    "\n",
    "* After Deployment\n",
    "    * Monitoring and Alerting\n",
    "        * Check logs and alerts from failures\n",
    "        * Check final storage location to ensure data is working as expected\n",
    "        * Communicate to Data Consumers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "* Source systems: where the raw data comes from.\n",
    "* To persist data: When data is persisted, it means that the exact same data can be retrieved by an application when it's opened again. There are different ways to persist data, which brings us to an important distinction: local storage versus remote storage.\n",
    "* Data Quality/Reliability/Integrity checks: Validate that data was correctly persisted in the different stages throughout the data pipeline.\n",
    "* Landing Zones: where the extracted and transformed data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buiding ETL Pipelines\n",
    "* Data Types of Source Systems - The Extract (E) stage: \n",
    "    * Tabular Data (Structured Data)\n",
    "        * Structured data is data that fits neatly into data tables and includes discrete data types such as numbers, short text, and dates.\n",
    "    * Non-tabular Data (Non-structured Data)\n",
    "        * Unstructured data doesn’t fit neatly into a data table because its size or nature: for example, audio and video files and large text documents.\n",
    "        * Sometimes, numerical or textual data can be unstructured because modeling it as a table is inefficient. For example, sensor data is a constant stream of numerical values, but creating a table with two columns—timestamp and sensor value—would be inefficient and impractical.\n",
    "* Common Landing Zones - The Load (L) stage:\n",
    "    * SQL Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular or Structured Data (SQL Database, Parquet, CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Step 1 - Extract (E)\n",
    "* Possible tabular (structured) sources systems for Data Pipelines:\n",
    "    * CSV\n",
    "    * Tabular source systems (with tabular file types): Parquet, SQL Databases (dynamic stores)\n",
    "    * Within organizations: Data Lakes, Data Warehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Extract (E) - SQL Database and Parquet\n",
    "# Using modularity\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract(file_path):\n",
    "  \t# Ingest the data to a DataFrame\n",
    "    raw_data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return raw_data\n",
    "  \n",
    "raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\n",
    "\n",
    "def extract():\n",
    "    connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/sales\"\n",
    "    db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "    raw_data = pd.read_sql(\"SELECT * FROM sales WHERE quantity_ordered = 1\", db_engine)\n",
    "    \n",
    "    # Print the head of the DataFrame\n",
    "    print(raw_data.head())\n",
    "    \n",
    "    # Return the extracted DataFrame\n",
    "    return raw_data\n",
    "    \n",
    "# Call the extract() function\n",
    "raw_sales_data = extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Step 2 - Transform (T)\n",
    "* Using Pandas and Modularity to perform data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Transformation (T) - CSV and Parquet\n",
    "# Using modularity\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1) Extract data from the sales_data.parquet path using the extract() function defined in the extract step (but adpated to parquet format)\n",
    "raw_sales_data = extract(\"sales_data.csv\")\n",
    "\n",
    "def transform(raw_data):\n",
    "    # Convert the \"Order Date\" column to type datetime\n",
    "    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n",
    "    \n",
    "    # Only keep items under ten dollars\n",
    "    clean_data = raw_data.loc[raw_data[\"Price Each\"] < 10, :]\n",
    "    return clean_data\n",
    "\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Check the data types of each column\n",
    "print(clean_sales_data.dtypes)\n",
    "\n",
    "\n",
    "# 2) Extract data from the sales_data.parquet path using the extract() function defined in the extract step (but adpated to parquet format)\n",
    "raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\n",
    "def transform(raw_data):\n",
    "  \t# Only keep rows with `Quantity Ordered` greater than 1\n",
    "    clean_data = raw_data.loc[raw_data[\"Quantity Ordered\"] > 1, :]\n",
    "    \n",
    "    # Only keep columns \"Order Date\", \"Quantity Ordered\", and \"Purchase Address\"\n",
    "    clean_data = raw_data[[\"Order Date\", \"Quantity Ordered\", \"Purchase Address\"]]\n",
    "    \n",
    "    # Return the filtered DataFrame\n",
    "    return clean_data\n",
    "    \n",
    "transformed_sales_data = transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Step 3 - Load (L) - Data Persistence and Snapshots\n",
    "* Using Pandas and Modularity to persist data to files in different point throughout the pipeline\n",
    "    * to_parquet()\n",
    "    * to_json()\n",
    "    * to_sql()\n",
    "* Goal: load the transformed data to persistent storage\n",
    "* Quality checks:\n",
    "    * ensure the file is in the location: use the 'os' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Load (T) - CSV\n",
    "# Using modularity\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the data to a csv file with the index, no header and pipe separated\n",
    "def load(transformed_data, path_to_write):\n",
    "\t# Write the data to a CSV file with the index column and no headers and pipe separated\n",
    "\ttransformed_data.to_csv(path_to_write, index=True, header=False)\n",
    "\n",
    "    # Check to make sure the file exists\n",
    "    file_exists = os.path.exists(path_to_write)\n",
    "    if not file_exists:\n",
    "        raise Exception(f\"File does NOT exists at path {path_to_write}\")\n",
    "\n",
    "# Call the function to load the transformed data to persistent storage.\n",
    "load(transformed_sales_data, \"transformed_sales_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-tabular or Unstructured Data (Text, Audio, Image, Video, Spatial, IoT)\n",
    "* The idea is to extract and transform such features (raw, unstructured data) into a \"tabular\" format.\n",
    "* APIs, JSONs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Step 1 - Extract (E)\n",
    "* Possible non-tabular (unstructured) sources systems for Data Pipelines:\n",
    "    * Ingesting from a 3rd party: APIs (and JSONs) \n",
    "    * JSONs\n",
    "    * From the web: Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json library\n",
    "import json\n",
    "\n",
    "def extract(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        # Load the data from the JSON file\n",
    "        raw_data = json.load(json_file)\n",
    "    return raw_data\n",
    "\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "\n",
    "# Print the raw_testing_scores\n",
    "print(raw_testing_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Step 2 - Transform (T)\n",
    "* Parsing unstructured data: from non-tabular to DataFrame\n",
    "* JSONs\n",
    "    * To transform data stored in, for example, dictionaries, we need to loop over keys and values.\n",
    "    * This can be done by transforming keys-values to lists\n",
    "        * .keys (creates a list of keys), .values (creates a list of values), .items (generates a list of tuples)\n",
    "    * Instead of only iterating over keys-values, we can also extract values: .get (takes a key and returns the value)\n",
    "        * With nested data, you can use \".get\" twice\n",
    "    * The data will eventually be transformed into a \"list of lists\", thus it will be ready to become a Dataframe\n",
    "        * pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dictionary from a JSON file\n",
    "\n",
    "# Example of the \"nested_school_scores.json\" file\n",
    "#     {\n",
    "#     \"01M539\": {\n",
    "#         \"street_address\": \"111 Columbia Street\",\n",
    "#         \"city\": \"Manhattan\",\n",
    "#         \"scores\": {\n",
    "#               \"math\": 657,\n",
    "#               \"reading\": 601,\n",
    "#               \"writing\": 601\n",
    "#         }\n",
    "#   }, ...\n",
    "# }\n",
    "\n",
    "raw_testing_scores_keys = []\n",
    "raw_testing_scores_values = []\n",
    "    # \n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "\n",
    "print(raw_testing_scores_keys[0:3])\n",
    "    # ['02M260', '06M211', '01M539']\n",
    "print(raw_testing_scores_values[0:3])\n",
    "    # [{'street_address': '425 West 33rd Street', 'city': 'Manhattan', 'scores': {'math': None, 'reading': None, 'writing': None}}, {'street_address': '650 Academy Street', 'city': 'Manhattan', 'scores': {'math': None, 'reading': None, 'writing': None}}, {'street_address': '111 Columbia Street', 'city': 'Manhattan', 'scores': {'math': 657.0, 'reading': 601.0, 'writing': 601.0}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse nested data within a dictionary where one key-value element is another dict\n",
    "# Create a DataFrame\n",
    "\n",
    "normalized_testing_scores = []\n",
    "\n",
    "# Loop through each of the dictionary key-value pairs and build normalized_testing_scores, a \"list of lists\"\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\tnormalized_testing_scores.append([\n",
    "    \tschool_id,\n",
    "    \tschool_info.get(\"street_address\"),  # Pull the \"street_address\"\n",
    "    \tschool_info.get(\"city\"),\n",
    "    \tschool_info.get(\"scores\").get(\"math\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"reading\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"writing\", 0),\n",
    "    ])\n",
    "\n",
    "print(normalized_testing_scores) # a \"list of lists\"\n",
    "    # [\n",
    "\t# \t['02M260', '425 West 33rd Street', 'Manhattan', None, None, None], \n",
    "\t# \t['06M211', '650 Academy Street', 'Manhattan', None, None, None], \n",
    "\t# \t...\n",
    "\t# ]\n",
    "\n",
    "# Create a DataFrame from the normalized_testing_scores list\n",
    "normalized_data = pd.DataFrame(normalized_testing_scores)\n",
    "\n",
    "# Set the column names\n",
    "normalized_data.columns = [\"school_id\", \"street_address\", \"city\", \"avg_score_math\", \"avg_score_reading\", \"avg_score_writing\"]\n",
    "\n",
    "normalized_data = normalized_data.set_index(\"school_id\")\n",
    "print(normalized_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Other examples of transforming data after it has been loaded as a DataFrame\n",
    "    * Impute values for the missing information\n",
    "        * fillna()\n",
    "        * groupby()\n",
    "        * apply()\n",
    "            * sometimes, more advanced logic needs to be used in a transformation. The apply function lets you apply a user-defined function to a row or column of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with fillna()\n",
    "def transform(raw_data):\n",
    "\traw_data.fillna(\n",
    "    \tvalue={\n",
    "\t\t\t# Fill NaN values with column mean\n",
    "\t\t\t\"math_score\": raw_data[\"math_score\"].mean(),\n",
    "\t\t\t\"reading_score\": raw_data[\"reading_score\"].mean(),\n",
    "\t\t\t\"writing_score\": raw_data[\"writing_score\"].mean()\n",
    "\t\t}, inplace=True\n",
    "\t)\n",
    "\treturn raw_data\n",
    "\n",
    "clean_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the clean_testing_scores DataFrame\n",
    "print(clean_testing_scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with .loc[] and groupby()\n",
    "def transform(raw_data):\n",
    "\t# Use .loc[] to only return the needed columns\n",
    "\traw_data = raw_data.loc[:, [\"city\", \"math_score\", \"reading_score\", \"writing_score\"]]\n",
    "\t\n",
    "    # Group the data by city, return the grouped DataFrame\n",
    "\tgrouped_data = raw_data.groupby(by=[\"city\"], axis=0).mean()\n",
    "\treturn grouped_data\n",
    "\n",
    "# Transform the data, print the head of the DataFrame\n",
    "grouped_testing_scores = transform(raw_testing_scores)\n",
    "print(grouped_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with apply()\n",
    "# Uses a pre-defined, more complex function to apply it to an object\n",
    "\n",
    "def transform(raw_data):\n",
    "\t# Use the apply function to extract the street_name from the street_address\n",
    "    raw_data[\"street_name\"] = raw_data.apply(\n",
    "   \t\t# Pass the correct function to the apply method\n",
    "        find_street_name,\n",
    "        axis=1\n",
    "    )\n",
    "    return raw_data\n",
    "\n",
    "# Transform the raw_testing_scores DataFrame\n",
    "cleaned_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the cleaned_testing_scores DataFrame\n",
    "print(cleaned_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Step 3 - Load (L)\n",
    "* Loading data to a SQL database (ex: Postgres), a common Landing Zone.\n",
    "    * .to_sql()\n",
    "        * name=\"name_of_the_table\"\n",
    "        * con=db_engine: the engine connection createdwith SQLAlchemy to a Postgres database\n",
    "        * if_exists=\"append\": appends records to the table if the table exists.\n",
    "        * if_exists=\"replace\": overwrites records in the table with the current DataFrame.\n",
    "        * index=True and index_label=\"timestamps\": provides a index to the new table in the Postgres database and in this case it is a timestamp column.\n",
    "* Data Quality checks:\n",
    "    * Validate that data was correctly persisted in postgres\n",
    "        * Ensure it can be queried\n",
    "            * pd.read_sql()\n",
    "        * Make sure counts match\n",
    "        * Validate each row is present\n",
    "* A SQL Database is used because it's easier to connect to Data Visualization tools\n",
    "    * Data Consumers are used to writing SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of persisting data in a postgres (a SQL database)\n",
    "\n",
    "def load(clean_data, con_engine):\n",
    "    clean_data.to_sql(name=\"scores_by_city\", con=con_engine, if_exists=\"replace\", index=True, index_label=\"school_id\")\n",
    "    \n",
    "# Call the load function, passing in the cleaned DataFrame\n",
    "load(cleaned_testing_scores, db_engine)\n",
    "\n",
    "# Call query the data in the scores_by_city table, check the head of the DataFrame\n",
    "to_validate = pd.read_sql(\"SELECT * FROM scores_by_city\", con=db_engine)\n",
    "print(to_validate.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Data Pipeline / Monitoring Pipeline Performance\n",
    "* The idea of monitoring: to _alert on failure_.\n",
    "* Goal: to alert Data Engineers before Data Consumers discover the issues\n",
    "* The data pipeline should be monitored for changes to data and failures during execution\n",
    "    * Examples:\n",
    "        * Source systems fail to provide data\n",
    "        * Data types change\n",
    "        * Packages or tools become deprecated or functionality changes\n",
    "* Types of tests:\n",
    "    * End-to-end testing and Checkpoint testing\n",
    "    * Unit testing (pytest & @fixtures: test_ tags and assert clauses)\n",
    "* Types of monitoring:\n",
    "    * Logs: \n",
    "        * They are the foundation for all monotoring methods\n",
    "        * They are messages created and written during the execution of a pipeline\n",
    "    * Alerts\n",
    "* Logging module\n",
    "    * debug: insights into data dimensionality, type, variable values. \n",
    "    * info: basic information and checkpoints throughout the execution of a pipeline, such as notification about operationst that occur on the data.\n",
    "    * warning: when something unexpected happens (an exception has not necessarily occurred), example: unexpected number of rows, previously unseen data types.\n",
    "    * error: appears when an exception occurs that should halt the executionof a pipeline, example: data format has changed or it is unavailable.\n",
    "* try & except\n",
    "    * the try-except logic is great for the warning and error part of the logging procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Monitoring Data Pipeline \n",
    "# Using modularity\n",
    "\n",
    "# 1) Expanding the Extract (E) stage to include logging\n",
    "\n",
    "def extract(file_path):\n",
    "    return pd.read_parquet(file_path)\n",
    "\n",
    "# Update the pipeline to include a try block\n",
    "try:\n",
    "\t# Attempt to read in the file\n",
    "    raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\t\n",
    "# Catch the FileNotFoundError\n",
    "except FileNotFoundError as file_not_found:\n",
    "\t# Write an error-level log\n",
    "\tlogging.error(file_not_found)\n",
    "\n",
    "\n",
    "# 2) Expanding the Transformation (T) stage to include logging\n",
    "\n",
    "import logging\n",
    "\n",
    "def transform(raw_data):\n",
    "    # Any transformation goes here\n",
    "    return raw_data.loc[raw_data[\"Total Price\"] > 1000, :]\n",
    "    \n",
    "try:\n",
    "    clean_data = transform(raw_data)\n",
    "    logging.info(\"Successfully filtered DataFrame by 'Total Price'\")\n",
    "\n",
    "    # Log the dimension of the DataFrame before and after filtering\n",
    "    logging.debug(f\"Shape of the DataFrame before filtering: {raw_data.shape}\")\n",
    "    logging.debug(f\"Shape of DataFrame after filtering: {clean_data.shape}\")\n",
    "\n",
    "except KeyError as ke:\n",
    "    logging.warning(f\"{ke}: Cannot filter DataFrame by 'Total Price'\")\n",
    "    \n",
    "    # Create the \"Total Price\" column, transform the updated DataFrame\n",
    "    raw_data[\"Total Price\"] = raw_data[\"Price Each\"] * raw_data[\"Quantity Ordered\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Types of tests:\n",
    "    * End-to-end testing and Checkpoint testing\n",
    "    * Unit testing (pytest & @fixtures: test_ tags, isinstance() and assert clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Testing Data Pipeline - E (CSV) + T (new col + filters) + L (in a parquet)\n",
    "\n",
    "# End-to-End Testing - Test \"E-T-L\" 3 times and check if the shape is the same\n",
    "    # Goal: The transformed data should not be duplicated in the parquet file.\n",
    "    # Checkpoint 1: E+T+L 3 times\n",
    "    # Checkpoint 2: Ensure the data is loaded correctly\n",
    "\t# Checkpoint 3: Compare Dataframes\n",
    "\n",
    "# Checkpoint 1: Trigger the data pipeline (E+T+L) 3 times\n",
    "for attempt in range(0, 3):\n",
    "\tprint(f\"Attempt: {attempt}\")\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\t\n",
    "\t# Print the shape of the cleaned_tax_data DataFrame\n",
    "\tprint(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "\n",
    "# Checkpoint 2: Ensure the data is loaded correctly by reading in the loaded data and checking the shape\n",
    "loaded_data = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(f\"Final shape of cleaned data: {loaded_data.shape}\")\n",
    "\n",
    "# Checkpoint 3: Compare Dataframes\n",
    "print(clean_data.equals(loaded_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 - Testing Data Pipeline - Pytest (test_ tags and istance())\n",
    "# Unit Testing\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_transformed_data():\n",
    "    raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "    clean_tax_data = transform(raw_tax_data)\n",
    "    \n",
    "    # Assert that the transform function returns a pd.DataFrame\n",
    "    assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "    \n",
    "    # Assert that the clean_tax_data DataFrame has more columns than the raw_tax_data DataFrame\n",
    "    assert len(clean_tax_data.columns) > len(raw_tax_data.columns)\n",
    "\n",
    "\n",
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Assert that clean_tax_data takes is an instance of a string\n",
    "try:\n",
    "\tassert isinstance(clean_tax_data, str)\n",
    "except Exception as e:\n",
    "\tprint(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 - Testing Data Pipeline - Pytest (@pytest.fixture())\n",
    "# Unit Testing\n",
    "\n",
    "import pytest\n",
    "\n",
    "# Define a pytest fixture\n",
    "@pytest.fixture() # creates a pytest fixture called clean_tax_data\n",
    "def clean_tax_data():\n",
    "    raw_data = pd.read_csv(\"raw_tax_data.csv\")\n",
    "    clean_data = transform(raw_data)\n",
    "    return clean_data\n",
    "\n",
    "# Pass the fixture to the function\n",
    "def test_tax_rate(clean_tax_data):\n",
    "    # Assert values are within the expected range\n",
    "    assert clean_tax_data[\"tax_rate\"].max() <= 1 and clean_tax_data[\"tax_rate\"].min() >= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecting a Pipeline (Testing vs. Production Envs + Orchestration)\n",
    "* Building a Alerting and Monitoring Solution\n",
    "* Best Practices to Architecting a Pipeline (more information in the Jupyter Notebook jup_soft_eng_python)\n",
    "    * Separate scripts into:\n",
    "        * myscript.py (where the execution logic is located)\n",
    "        * xyz_utils.py (where the E+T+L functions definitions is in a separated location)\n",
    "        * Recall the folder structure when building packages\n",
    "    * Use Logs/Logging and try-except block\n",
    "* Testing Environment\n",
    "    * DEs can play with sample data without worrying about breaking data sources.\n",
    "* Production Environment\n",
    "    * Data Engineers need to make sure that their pipelines can run consistently on a schedule, have access to a flexible quantity of resources, and alert on failure. To do this, Data Engineers will often look outside of a Python script to an orchestration and ETL tool, such as Airflow.\n",
    "    * It's important to monitor the performance of a pipeline when running in production.\n",
    "    * We'll practice running a pipeline end-to-end, while monitoring for exceptions and logging performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Environment to Run a Pipeline end-to-end\n",
    "# Recall\n",
    "    # working_dir\n",
    "    # ├── setup.py\n",
    "    # ├── requirements.txt\n",
    "    # ├── etl_pipeline_package (my_package)\n",
    "    # │    ├── __init__.py\n",
    "    # │    ├── pipeline_utils.py (xyz.utils.py, the module)\n",
    "    # └── etl_pipeline.py (my_script.py, the execution logic script)\n",
    "\n",
    "# Import extract, transform, and load functions from pipeline_utils\n",
    "# Import the logging package\n",
    "import logging\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "try:\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "    \n",
    "\tlogging.info(\"Successfully extracted, transformed and loaded data.\")  # Log a success message.\n",
    "    \n",
    "except Exception as e:\n",
    "\tlogging.error(f\"Pipeline failed with error: {e}\")  # Log failure message\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
